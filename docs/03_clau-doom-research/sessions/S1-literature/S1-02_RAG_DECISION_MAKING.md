# S1-02: RAG for Decision-Making ë¬¸í—Œ ìˆ˜ì§‘

> **ì„¸ì…˜**: S1 (ë¬¸í—Œ ìˆ˜ì§‘)
> **ìš°ì„ ìˆœìœ„**: ğŸ”´ critical
> **ì˜ì¡´ì„±**: ì—†ìŒ
> **ìƒíƒœ**: â¬œ ë¯¸ì‹œì‘

---

## ëª©ì 

RAGë¥¼ "ì‹¤ì‹œê°„ í–‰ë™ ì„ íƒ"ì— ì“°ëŠ” ê²ƒì´ í”„ë¡œì íŠ¸ì˜ ê°€ì¥ ë¹„ì •í†µì ì¸ ì„¤ê³„ ê²°ì •ì´ë‹¤. í˜„ì¬ ë¬¸í—Œ(Reflexion, Voyager)ì€ self-reflection/skill library ìª½ì´ë©°, ê²€ìƒ‰ ê¸°ë°˜ ì˜ì‚¬ê²°ì • ìì²´ë¥¼ ë‹¤ë£¨ëŠ” ì„ í–‰ ì—°êµ¬ê°€ ë¶€ì¡±í•˜ë‹¤. ì´ ì¶•ì„ ë³´ê°•í•˜ì—¬ "RAGê°€ RLì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ê°€"ë¼ëŠ” ì£¼ì¥ì˜ í•™ìˆ ì  ê·¼ê±°ë¥¼ í™•ë³´í•œë‹¤.

---

## ìˆ˜ì§‘ ëŒ€ìƒ ë¬¸í—Œ ì¹´í…Œê³ ë¦¬

### A. Retrieval-Augmented Decision Making

**ì™œ í•„ìš”í•œê°€**: clau-doomì˜ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ â€” OpenSearch kNN ê²€ìƒ‰ â†’ Rust ìŠ¤ì½”ì–´ë§ â†’ ì•¡ì…˜ ì„ íƒ â€” ì´ "ê²€ìƒ‰ìœ¼ë¡œ ì •ì±…ì„ ëŒ€ì²´í•œë‹¤"ëŠ” ì£¼ì¥ì˜ ì§ì ‘ì  ì„ í–‰ ì—°êµ¬.

**íƒìƒ‰ í‚¤ì›Œë“œ**:
- retrieval-augmented decision making
- retrieval-augmented reinforcement learning
- kNN policy / nearest neighbor policy
- episodic memory reinforcement learning

**ê¸°ëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸**:
- Humphreys et al. (2022) â€” Large-Scale Retrieval for Reinforcement Learning
- Goyal et al. (2022) â€” Retrieval-Augmented Reinforcement Learning
- Blundell et al. (2016) â€” Model-Free Episodic Control (NEC)
- Pritzel et al. (2017) â€” Neural Episodic Control

**clau-doom ì—°ê²°ì **:
- OpenSearch kNN ê²€ìƒ‰ â‰ˆ episodic memoryì˜ k-nearest lookup
- ì „ëµ ë¬¸ì„œì˜ success_rate â‰ˆ episodic controlì˜ Q-value ì¶”ì •
- ìœŒìŠ¨ ìŠ¤ì½”ì–´ í•˜í•œ â‰ˆ ë¶ˆí™•ì‹¤ì„± ë³´ì •ëœ ê°€ì¹˜ ì¶”ì •

---

### B. Decision Transformer / Sequence Model ê¸°ë°˜ ì˜ì‚¬ê²°ì •

**ì™œ í•„ìš”í•œê°€**: Transformerë¥¼ ì˜ì‚¬ê²°ì •ì— ì“°ëŠ” ì ‘ê·¼ì€ RL ëŒ€ì•ˆìœ¼ë¡œì„œì˜ sequence modeling. clau-doomì˜ "ê²½í—˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì„œ ì˜ì‚¬ê²°ì •"ê³¼ êµ¬ì¡°ì  ìœ ì‚¬ì„±ì´ ìˆìŒ.

**íƒìƒ‰ í‚¤ì›Œë“œ**:
- Decision Transformer
- offline reinforcement learning sequence model
- in-context learning reinforcement learning

**ê¸°ëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸**:
- Chen et al. (2021) â€” Decision Transformer
- Janner et al. (2021) â€” Trajectory Transformer
- Lee et al. (2024) â€” Supervised Pretraining Can Learn In-Context RL
- Laskin et al. (2022) â€” In-context RL with Algorithm Distillation

**clau-doom ì—°ê²°ì **:
- Decision Transformerê°€ "ê³¼ê±° trajectoryë¥¼ ì¡°ê±´ìœ¼ë¡œ í–‰ë™ ìƒì„±" â†’ clau-doomì€ "ê³¼ê±° ì „ëµ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì„œ í–‰ë™ ì„ íƒ"
- ì°¨ì´: DTëŠ” í•™ìŠµëœ ëª¨ë¸, clau-doomì€ ê²€ìƒ‰ + ìŠ¤ì½”ì–´ë§ (í•™ìŠµ ì—†ìŒ)

---

### C. RETRO / Retrieval-Augmented Generation ê³„ì—´

**ì™œ í•„ìš”í•œê°€**: RAGì˜ ì›ë¥˜ ì—°êµ¬. ìƒì„±ì´ ì•„ë‹Œ ì˜ì‚¬ê²°ì •ì— RAGë¥¼ ì“°ëŠ” clau-doomì˜ í™•ì¥ì´ ì–´ë–¤ ìœ„ì¹˜ì¸ì§€ í¬ì§€ì…”ë‹ í•„ìš”.

**íƒìƒ‰ í‚¤ì›Œë“œ**:
- RETRO retrieval enhanced transformer
- RAG beyond generation
- retrieval augmented agent

**ê¸°ëŒ€ ìˆ˜ì§‘ ë…¼ë¬¸**:
- Borgeaud et al. (2022) â€” RETRO
- Lewis et al. (2020) â€” RAG ì›ë…¼ë¬¸
- RAGë¥¼ ì—ì´ì „íŠ¸ ì˜ì‚¬ê²°ì •ì— ì ìš©í•œ ìµœì‹  ì—°êµ¬

**clau-doom ì—°ê²°ì **:
- RAG ì›ë…¼ë¬¸ì´ "ìƒì„± í’ˆì§ˆ í–¥ìƒ"ì„ ìœ„í•œ ê²ƒ â†’ clau-doomì€ "í–‰ë™ í’ˆì§ˆ í–¥ìƒ"ìœ¼ë¡œ í™•ì¥
- RETROì˜ chunked retrieval â‰ˆ clau-doomì˜ ìƒí™©ë³„ ì „ëµ ë¬¸ì„œ ê²€ìƒ‰

---

## íŒ€ êµ¬ì„± (Agent Teams)

| ì—­í•  | ë‹´ë‹¹ ë²”ìœ„ |
|------|----------|
| Lead | ë¬¸í—Œ íƒìƒ‰ ì´ê´„, clau-doomê³¼ì˜ í¬ì§€ì…”ë‹ ì •ë¦¬ |
| Sub-agent A | ì¹´í…Œê³ ë¦¬ A: Retrieval-Augmented RL ì„œì¹˜ |
| Sub-agent B | ì¹´í…Œê³ ë¦¬ B + C: DT ê³„ì—´ + RETRO ê³„ì—´ ì„œì¹˜ |

---

## ì‚°ì¶œë¬¼ í˜•ì‹

```
### [ë…¼ë¬¸ ì œëª©] (ì €ì, ì—°ë„, í•™íšŒ)
- **í•µì‹¬ ë‚´ìš©**: 1~2ë¬¸ì¥ ìš”ì•½
- **clau-doom ì—°ê²°ì **: ìš°ë¦¬ í”„ë¡œì íŠ¸ì™€ì˜ ê´€ê³„
- **ì°¨ë³„ì **: ìš°ë¦¬ê°€ ë‹¤ë¥´ê²Œ í•˜ëŠ” ë¶€ë¶„
- **ì°¸ê³  ì—¬ë¶€**: ì§ì ‘ ì¸ìš© / ë°°ê²½ ì°¸ê³  / ë°©ë²•ë¡  ì°¨ìš©
```

---

## ì™„ë£Œ ê¸°ì¤€

- [ ] ì¹´í…Œê³ ë¦¬ Aì—ì„œ ìµœì†Œ 3í¸ ìˆ˜ì§‘
- [ ] ì¹´í…Œê³ ë¦¬ Bì—ì„œ ìµœì†Œ 2í¸ ìˆ˜ì§‘
- [ ] ì¹´í…Œê³ ë¦¬ Cì—ì„œ ìµœì†Œ 2í¸ ìˆ˜ì§‘
- [ ] "RAGê°€ RLì„ ëŒ€ì²´í•œë‹¤"ëŠ” ì£¼ì¥ì˜ í•™ìˆ ì  í¬ì§€ì…”ë‹ ë¬¸ì¥ ì‘ì„±
- [ ] Lead ê²€ìˆ˜ ì™„ë£Œ
