# Core Content: Methodology and Results

## 3. Methodology

We describe our agent architecture, experimental framework, and evaluation environment. All experiments follow a pre-registered Design of Experiments (DOE) protocol with fixed seeds, ANOVA-based analysis, and complete residual diagnostics. The full audit trail---from hypothesis to finding---is maintained across 29 experiments.

### 3.1 Agent Architecture

Our agent employs a four-level hierarchical decision architecture designed to achieve sub-100ms decision latency without real-time LLM inference. Each level operates within a strict latency budget, and higher levels are consulted only when lower levels do not produce a decisive action.

**Level 0 (L0): Hardcoded Rules** ($< 1\text{ms}$). Implemented in Rust, L0 encodes basic reflexes: dodge when health drops below 20, reload when ammunition reaches zero, and attack the nearest visible enemy otherwise. These rules provide a deterministic behavioral floor that guarantees minimum competence regardless of higher-level availability.

**Level 1 (L1): Episode Cache** ($< 10\text{ms}$). A per-agent DuckDB instance stores the most recent 100 episodes of play history. L1 retrieves frequently accessed behavioral patterns---notably periodic action sequences such as \texttt{burst\_3} (attack for 3 ticks, then reposition for 3 ticks). L1 patterns are generated by Claude Code during offline retrospection and cached for real-time access.

**Level 2 (L2): Strategy Retrieval** ($< 100\text{ms}$). An OpenSearch instance indexes strategy documents generated by Claude Code after each episode. During gameplay, L2 performs $k$-nearest-neighbor vector search ($k{=}5$) over the strategy corpus, scores retrieved documents against the current game state, and selects the highest-scoring tactic. Strategy documents encode tactical knowledge at varying granularity: from low-level action mappings ("when enemies approach, strafe left") to meta-strategies ("use aggressive tactics when health is high").

**Level 3 (L3): Retrospection** (offline, seconds). After each episode concludes, Claude Code analyzes the episode outcome, generates new strategy documents, and refines existing ones. L3 never executes during gameplay---it operates exclusively between episodes, eliminating the latency penalties that have constrained prior LLM-based game agents.

The architecture's core hypothesis was: $\text{Agent Skill} = \text{Document Quality} \times \text{Scoring Accuracy}$. That is, performance should improve monotonically as (a) strategy documents become higher quality through retrospective refinement and (b) the Rust scoring engine more accurately matches documents to game states.

**Game interface.** A Python binding to the VizDoom API translates between the decision hierarchy and the game engine. Action functions produce discrete actions at each game tick (28.6ms at 35 fps). Metrics recorded per episode include kills (count of enemies eliminated), survival time (seconds until agent death), and kill rate (kills per minute of survival).

### 3.2 Experimental Framework

We adopt Design of Experiments (DOE) methodology from quality engineering, replacing ad-hoc hyperparameter search with structured factorial designs, ANOVA-based inference, and effect size quantification. Our experimental program follows a phased progression:

**Phase 0: Screening (OFAT).** One-Factor-At-a-Time designs identify which factors exhibit large main effects. DOE-001 through DOE-010 screened agent architecture variants, strategy types, and scenario characteristics. This phase identified movement as a potentially important factor and eliminated several candidate parameters (memory weight, strength weight) as non-influential.

**Phase 1: Factorial Analysis.** Full and fractional factorial designs ($2^k$ and multi-level) quantify main effects, interactions, and effect sizes. DOE-011 through DOE-021 tested strategy combinations, action space variants, cross-difficulty robustness, and evolutionary optimization. This phase established the convergence zone phenomenon---that all strategies meeting minimum conditions achieve statistically equivalent performance.

**Phase 2: Hypothesis Testing.** Three independent experiments (DOE-022, DOE-024, DOE-026) tested the core L2 RAG hypothesis using different retrieval implementations and action spaces. All three produced null results, falsifying the core thesis.

**Phase 3: Constraint Discovery.** DOE-027 through DOE-029 characterized the rate-time compensation mechanism and confirmed movement as the sole performance determinant.

**Statistical protocol.** Every experiment adheres to a rigorous statistical pipeline:

1. *Fixed seeds.* Control and treatment conditions use identical seed sets, enabling paired comparisons and guaranteeing reproducibility. Minimum 30 seeds per condition provides statistical power $\geq 0.80$ for medium effect sizes ($f = 0.25$).

2. *ANOVA with diagnostics.* We report the full ANOVA table (sums of squares, degrees of freedom, $F$-statistic, $p$-value) accompanied by three residual diagnostics: normality (Anderson-Darling test), equal variance (Levene test), and independence (run-order inspection). When parametric assumptions are violated, we confirm results with non-parametric alternatives (Kruskal-Wallis, Mann-Whitney $U$).

3. *Effect sizes.* Every significant result reports partial $\eta^2$ (proportion of variance explained) and Cohen's $d$ (standardized mean difference) with 95\% confidence intervals. We interpret effect sizes using standard thresholds: small ($d = 0.2$, $\eta^2 = 0.01$), medium ($d = 0.5$, $\eta^2 = 0.06$), and large ($d = 0.8$, $\eta^2 = 0.14$).

4. *Trust classification.* Findings receive a trust level: HIGH ($p < 0.01$, $n \geq 50$, clean diagnostics), MEDIUM ($p < 0.05$, $n \geq 30$), LOW ($p < 0.10$ or diagnostic violations), or UNTRUSTED (no statistical test or $p \geq 0.10$). Only HIGH and MEDIUM findings are adopted.

5. *Audit trail.* Every finding traces through a complete chain: hypothesis (HYPOTHESIS\_BACKLOG) $\to$ experiment order (EXPERIMENT\_ORDER) $\to$ experiment report (EXPERIMENT\_REPORT) $\to$ finding (FINDINGS). No result is reported without this chain.

### 3.3 VizDoom Environment

We evaluate on VizDoom's \texttt{defend\_the\_line} scenario, selected through systematic comparison against alternative scenarios.

**Scenario characteristics.** The agent faces a corridor of continuously spawning single-hit enemies (imps). The agent occupies a fixed longitudinal position and can turn, strafe laterally, and fire. Episodes terminate upon agent death. The \texttt{doom\_skill} parameter (1--5) controls enemy aggressiveness, with skill 3 (Normal) as the default.

**Action spaces.** We test two action space configurations:
- *3-action:* $\{$TURN\_LEFT, TURN\_RIGHT, ATTACK$\}$. The agent can aim and fire but cannot physically reposition.
- *5-action:* $\{$TURN\_LEFT, TURN\_RIGHT, MOVE\_LEFT, MOVE\_RIGHT, ATTACK$\}$. Adds lateral strafing, enabling projectile avoidance through physical displacement.

**Response variables.** We measure three complementary metrics: (i) kills---total enemies eliminated per episode; (ii) survival time---seconds from episode start to agent death; and (iii) kill rate---kills per minute of survival, measuring offensive efficiency independent of episode duration.

**Scenario selection rationale.** Early experiments (DOE-007, DOE-008) compared \texttt{defend\_the\_center} and \texttt{defend\_the\_line}. The former produces kills in the range 0--3 with near-zero variance, yielding no statistical discrimination between strategies ($F(4,145) = 1.579$, $p = 0.183$, $\eta^2 = 0.042$). The latter produces kills in the range 4--26 with sufficient variance for meaningful ANOVA ($F(4,145) = 5.256$, $p < 0.001$, $\eta^2 = 0.127$). We therefore adopt \texttt{defend\_the\_line} as the standard evaluation scenario for all subsequent experiments.


## 4. Results

We present results across four phases spanning 29 DOEs and 5,010 total episodes. Phase 0--1 validates infrastructure and maps the strategy landscape. Phase 2 tests and falsifies the core RAG hypothesis. Phase 3 discovers rate-time compensation and confirms movement as the sole performance determinant.

### 4.1 Infrastructure Validation and Strategy Exploration (DOE-001--020)

**Infrastructure validation (DOE-001--004).** Initial experiments revealed a critical data pipeline error: the AMMO2 game variable was incorrectly mapped as KILLCOUNT in the DuckDB schema, invalidating the first four experiments. After correction with real VizDoom execution, we discovered that the full agent (L0+L1+L2) produces *identical* outcomes to the rule-only agent (L0 only) at default parameters---both achieve 26.0 kills with zero variance in \texttt{defend\_the\_center}. This early null result (F-002, INVALIDATED) provided the first indication that the L1 and L2 layers contribute no behavioral differentiation at default settings.

**Scenario selection (DOE-007--008).** A paired comparison using identical five-level designs on two scenarios established \texttt{defend\_the\_line} as the standard evaluation environment. On \texttt{defend\_the\_center}, architecture had no significant effect ($F(4,145) = 1.579$, $p = 0.183$, $\eta^2 = 0.042$; power = 0.49). On \texttt{defend\_the\_line}, architecture was significant ($F(4,145) = 5.256$, $p < 0.001$, $\eta^2 = 0.127$; power = 0.97). The discriminability ratio improved 1.7$\times$, effect size increased 3$\times$, and residual diagnostics shifted from all-fail to all-pass (F-012).

**Agent parameter null results (DOE-009).** A $3^2$ factorial design testing memory weight $\times$ strength weight on \texttt{defend\_the\_line} produced uniformly null results. Memory weight: $F(2,261) = 0.306$, $p = 0.736$, $\eta^2 = 0.002$. Strength weight: $F(2,261) = 2.235$, $p = 0.109$, $\eta^2 = 0.017$. Interaction: $F(4,261) = 0.365$, $p = 0.834$, $\eta^2 = 0.006$ (F-013, F-014, F-015). These results invalidated earlier mock-data findings that had attributed 41.5\% of variance to memory ($p < 0.0001$ in mock data). **Agent-level parameters have no detectable effect in real gameplay.**

**Strategy landscape (DOE-010--020).** Systematic testing of behavioral strategies revealed a consistent pattern across 11 experiments:

- *L0-only is universally worst.* The pure-reflex strategy (always attack nearest enemy) was significantly worse than all alternatives across three independent experiments with different seed sets: DOE-008 ($d = 0.938$), DOE-010 ($d = 0.654$), DOE-019 ($d = 0.83$--$1.48$). The mechanism is tunnel vision: L0-only commits to a single enemy without lateral scanning (F-010, F-034).

- *Random matches structured strategies in 3-action space.* Random action selection is statistically indistinguishable from all structured strategies: $F(3,116) = 0.517$, $p = 0.671$ (F-018). The 3-action space is too constrained for intelligent strategies to outperform uniform randomness.

- *Strafing trades kill rate for survival.* Expanding from 3 to 5 actions reduces kill rate by 3.18 kr/min ($d = 0.523$, $p = 0.003$) but increases survival by 63\% ($\eta^2 = 0.225$), producing more total kills (F-020, F-023, F-024).

- *Compound actions confer no benefit.* Simultaneous multi-action commands (attack+turn on the same tick) produce identical results to sequential commands ($d = 0.000$; F-025). VizDoom's weapon cooldown (~12 ticks, ~340ms) absorbs all timing differences between strategies (F-043).

- *Best-of-breed strategies.* \texttt{burst\_3} (3 attacks, 1 reposition) achieves the highest total kills (15.40; F-036). \texttt{adaptive\_kill} (state-dependent switching) achieves the highest kill rate (46.18 kr/min; F-032). Both form a two-member Pareto front; all other strategies are dominated (F-039, F-041).

### 4.2 Core Thesis Falsification (DOE-022, DOE-024, DOE-026)

We tested the central hypothesis---that L2 RAG strategy retrieval improves agent performance---through three independent experiments spanning two action spaces and three retrieval implementations.

**DOE-022: L2 with coarse action mapping.** A $3 \times 2$ factorial design (decision mode $\times$ doom\_skill) tested L2 retrieval with direct tactic-to-action mapping across four document quality levels. Decision mode had no effect on kills: $F(2,174) = 0.268$, $p = 0.765$, $\eta^2 = 0.003$. Furthermore, high-quality and low-quality strategy documents produced *perfectly identical* episode outcomes (30/30 episodes matched; $d = 0.000$; F-050). The coarse 3-action mapping collapses all tactical distinctions into the same action distribution: nearly all tactics map to ATTACK regardless of document content.

**DOE-024: L2 meta-strategy selection.** A $4 \times 3$ factorial design tested a refined L2 implementation that selects between pre-validated strategies (\texttt{burst\_3} vs.\ \texttt{adaptive\_kill}) based on situation tags (health, ammo, kill count). Decision mode showed no effect: $F(3,348) = 1.001$, $p = 0.393$, $\eta^2 = 0.009$. All planned contrasts were non-significant (all $p > 0.4$, all $d < 0.12$). The pre-filtered strategy pool eliminates selection value: when all candidate strategies perform equivalently (F-035), choosing between them provides no advantage (F-057).

**DOE-026: L2 in 5-action space.** A five-condition OFAT design tested L2 retrieval in the richer 5-action space, where strategy differentiation exists (F-062). Three L2 implementations (cached lookup, live OpenSearch query, random rotation among top strategies) were compared against two fixed baselines. Decision mode had no effect: $F(4,145) = 0.206$, $p = 0.935$, $\eta^2 = 0.006$. This is the smallest effect size in the entire 29-DOE program. The RAG selector was numerically the worst performer (16.57 kills vs.\ group mean 17.15), suggesting that query overhead may cause slight regression (F-069).

**Synthesis (F-070): Thesis falsification.** Across three independent tests with cumulative $N = 450$ episodes, using different action spaces (3-action and 5-action), different retrieval granularities (tactic-level, meta-strategy, cached/live), and different document pools (curated vs.\ random), L2 RAG retrieval produces no measurable performance benefit. The hypothesis "Agent Skill $=$ Document Quality $\times$ Scoring Accuracy" is **falsified** for the \texttt{defend\_the\_line} scenario.

### 4.3 Rate-Time Compensation Discovery (DOE-027, DOE-028)

Having falsified the RAG hypothesis, we investigated *why* tactical variation produces null results. Two experiments reveal a fundamental environment constraint.

**DOE-027: Attack ratio gradient.** A seven-level OFAT design varied attack probability from 20\% to 80\% in 10\% increments within the 5-action space. Kills were invariant to attack ratio: $F(6,203) = 0.617$, $p = 0.717$, $\eta^2 = 0.018$ (F-071). However, the component metrics diverged sharply. Survival time decreased with attack ratio ($-7.77$s per 10\% increase; $p = 0.016$; F-072), while kill rate increased monotonically ($F(6,203) = 3.736$, $p = 0.002$, $\eta^2 = 0.099$; Jonckheere-Terpstra $z = 7.084$, $p < 0.001$; F-073). The two trends cancel precisely:

$$\text{kills} \approx \text{kill\_rate} \times \frac{\text{survival\_time}}{60}$$

At 20\% attack: $\text{kr} = 36.5/\text{min} \times 26.2\text{s} / 60 = 15.9$ kills. At 80\% attack: $\text{kr} = 42.0/\text{min} \times 21.3\text{s} / 60 = 14.9$ kills. The product varies by only 6\% across a 4$\times$ range of attack probability (F-074).

This **rate-time compensation** represents a conservation law of the scenario: total kills are an environment-determined quantity that tactical allocation between offense and defense merely redistributes between kill rate and survival time.

**DOE-028: Temporal structure.** A five-condition OFAT design held attack ratio constant at 50\% and varied temporal grouping: random frame-by-frame interleaving, and deterministic burst cycles of length 2, 3, 5, and 10 ticks. Kills were invariant to temporal structure: $F(4,145) = 1.017$, $p = 0.401$, $\eta^2 = 0.027$ (F-076). All four planned contrasts were non-significant ($p = 0.636$ to $p = 0.893$). Rate-time compensation held with remarkable precision: the $\text{kr} \times \text{survival} / 60$ ratio ranged from 0.980 to 1.003 across all five patterns (F-078).

**Full tactical invariance (F-077).** The combined evidence from DOE-027 ($N = 210$) and DOE-028 ($N = 150$) establishes that **neither the proportion of attacks nor their temporal distribution affects total kills** in the 5-action \texttt{defend\_the\_line} environment. This invariance is a direct consequence of the rate-time compensation constraint: any tactical reallocation between offense and defense is perfectly offset, preserving a fixed kill budget per episode.

### 4.4 Movement as Sole Determinant (DOE-029)

The final experiment tests whether movement---the one factor that varies *between* movement classes rather than *within*---is indeed the sole performance lever.

**Design.** A $2^2$ full factorial design crossed action pattern (random\_50: 50\% attack with strafing vs.\ pure\_attack: 100\% attack without strafing) with health override (emergency dodge when health $< 20$: enabled vs.\ disabled). Each cell contained 30 episodes ($N = 120$ total).

**Movement effect (F-079).** Action pattern produced the largest effect in the entire 29-DOE program: $F(1,116) = 58.402$, $p < 0.001$, $\eta^2 = 0.332$, $d = 1.408$. Agents with movement achieved 15.25 $\pm$ 5.74 kills and 24.4s survival; agents without movement achieved 8.85 $\pm$ 2.99 kills and 15.3s survival. Kruskal-Wallis confirmed ($H(3) = 50.802$, $p < 0.001$).

**Health override null (F-080).** The emergency dodge mechanism had no effect: $F(1,116) = 0.784$, $p = 0.378$, $\eta^2 = 0.004$, $d = -0.134$. This confound was present in all DOE-025 through DOE-028 experiments; DOE-029 demonstrates it was irrelevant throughout.

**No interaction (F-081).** The interaction between pattern and override was non-significant: $F(1,116) = 0.987$, $p = 0.322$, $\eta^2 = 0.006$. The override is equally irrelevant for both movers and non-movers.

**Kill rate invariance (F-083).** Critically, kill rate did *not* differ between movement conditions: random\_50 achieved $42.2 \pm 5.1$ kr/min vs.\ pure\_attack at $40.8 \pm 5.9$ kr/min ($p = 0.180$, $d = 0.248$). Movement does not reduce killing efficiency---it provides survival time at essentially zero offensive cost. The mechanism is orthogonal: turning (which aims at enemies) is independent of strafing (which displaces the agent laterally). Strafing makes the agent harder to hit by enemy projectiles without interfering with the agent's own offensive output.

**Rate-time compensation breaks at the movement boundary (F-082).** Within the movement class, the $\text{kr} \times \text{survival}$ product is 17.17. Within the non-movement class, the product is 10.38---a 65\% gap. Rate-time compensation holds *within* each class but fails *between* them. Movement provides "free" survival that is not offset by reduced kill rate, breaking the conservation law that constrains tactical variation within a movement class.

**Synthesis.** Movement is the sole performance determinant in \texttt{defend\_the\_line}. Across 29 experiments testing attack ratio (DOE-027), temporal structure (DOE-028), RAG selection (DOE-022, 024, 026), health override (DOE-029), memory and strength weights (DOE-009), compound actions (DOE-012), scenario difficulty (DOE-023), and evolutionary optimization (DOE-021), only one factor produces a significant, large, and replicated effect on total kills: whether the agent's action space includes lateral movement. The simplest effective agent---random action selection over the 5-action space---is statistically indistinguishable from the most sophisticated RAG-augmented, state-dependent, evolutionarily optimized architecture.
