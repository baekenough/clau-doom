\documentclass{article}
\usepackage[preprint]{neurips_2025}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{longtable}

\title{Appendix: Systematic Experimental Evidence}

\author{
  Supplementary Material for: \\
  \textit{Movement Is All You Need: How 45 Systematic Experiments} \\
  \textit{Falsified RAG-Based FPS Game Agent Optimization}
}

\begin{document}

\maketitle

\appendix

\section{Complete DOE Summary Table}
\label{app:doe-summary}

This table presents all 45 Design of Experiments (DOE) conducted across Phase 0 (infrastructure validation), Phase 1 (systematic exploration), Phase 2 (optimization and falsification), Phase 3 (confirmation and replication), and Phase 4 (new scenarios, evolution, and difficulty mapping).

\begin{small}
\begin{longtable}{@{}p{0.6cm}p{0.6cm}p{1.2cm}p{1.8cm}p{0.8cm}p{3.5cm}p{1.3cm}p{1.1cm}p{1.4cm}@{}}
\toprule
DOE & Phase & Design & Factors & Episodes & Key Finding & Primary p-value & Effect Size & Result \\
\midrule
\endfirsthead
\toprule
DOE & Phase & Design & Factors & Episodes & Key Finding & Primary p-value & Effect Size & Result \\
\midrule
\endhead
\midrule
\multicolumn{9}{r}{\textit{Continued on next page}} \\
\endfoot
\bottomrule
\endlastfoot
001 & 0 & OFAT (3) & Arch (rand/rule/full) & 210 & Full $\gg$ Random & $<0.001$ & $d=5.28$ & Mock data bug \\
001-R & 0 & OFAT (3) & Arch (real VizDoom) & 210 & Full=Rule$\gg$Rand & $<0.001$ & $d=6.84$ & L1/L2 zero diff \\
002 & 0 & $2\times2$ & Memory$\times$Strength & 150 & INVALIDATED & --- & --- & AMMO2 bug \\
003 & 0 & --- & --- & 0 & Not executed & --- & --- & Superseded \\
004 & 0 & --- & --- & 0 & Not executed & --- & --- & Superseded \\
005 & 1a & $2\times2$ & Memory$\times$Strength [0.7,0.9] & 150 & Zero variance & 1.000 & $\eta^2=0.000$ & Plateau \\
006 & 1a & Bug fix & KILLCOUNT mapping & 150 & Fixed, kills 0--3 & --- & --- & dfc too easy \\
007 & 1b & OFAT (3) & Scenario comparison & 210 & dtl $\gg$ center & $<0.001$ & --- & Scenario select \\
008 & 1b & OFAT (5) & Arch on dtl & 150 & L0\_only worst & 0.0006 & $\eta^2=0.127$ & First sig. \\
009 & 1b & $3\times3$ & Memory$\times$Strength (real) & 270 & All NULL & $>0.10$ & $\eta^2<0.02$ & Params irrelevant \\
010 & 1c & OFAT (3) & Arch replication & 90 & Strategy matters & $<0.001$ & $\eta^2=0.267$ & DOE-008 replic. \\
011 & 1c & OFAT (5) & 5-action strategies & 150 & Rate-total tradeoff & $<0.001$ & $\eta^2=0.225$ & Strafing trades \\
012 & 1d & OFAT (4) & 3-action patterns & 120 & Random$\approx$struct & 0.671 & $\eta^2=0.013$ & H-014 rejected \\
013 & 1d & OFAT (4) & 3-action (replic.) & 120 & Replicates DOE-012 & 0.581 & $\eta^2=0.016$ & Confirmed \\
014 & 1d & OFAT (5) & 5-action intelligent & 150 & Random competitive & 0.039 & $\eta^2=0.067$ & H-015 partial \\
015 & 1d & OFAT (4) & Attack ratio sweep & 120 & No effect 50--100\% & 0.812 & $\eta^2=0.008$ & Attack irrelevant \\
016 & 1d & OFAT (3) & L0 health threshold & 90 & Threshold 0 optimal & 0.003 & $\eta^2=0.124$ & Dodge hurts \\
017 & 1e & $4\times3$ & Scenario exploration & 360 & dtl confirmed & $<0.001$ & --- & basic/corridor bad \\
018 & 1e & OFAT (4) & Compound actions & 120 & No benefit & 0.547 & $\eta^2=0.018$ & Compound=simple \\
019 & 1e & OFAT (5) & Compound+attack\_only & 150 & attack\_only deficit & 0.012 & $\eta^2=0.087$ & L0\_only worst \\
020 & 1e & OFAT (5) & Best-of-breed & 150 & burst\_3$>$adaptive & $<0.001$ & $\eta^2=0.199$ & Multi-obj needed \\
021 & 2a & OFAT (6) & Generational evolution & 180 & burst\_3 optimal & $<0.001$ & $\eta^2=0.382$ & Converges Gen 2 \\
022 & 2a & $3\times2$ & L2 RAG$\times$doom\_skill & 180 & L2 RAG null (1st) & 0.765 & $\eta^2=0.003$ & First L2 fail \\
023 & 2a & $4\times3$ & Strategy$\times$difficulty & 360 & doom\_skill dominant & $<0.001$ & $\eta^2=0.720$ & Env dominates \\
024 & 2b & $3\times2$ & L2 meta-strat$\times$skill & 180 & L2 null (2nd) & 0.598 & $\eta^2=0.006$ & Second L2 fail \\
025 & 2b & OFAT (5) & 5-action optimization & 150 & Strategy differentiates & $<0.001$ & $\eta^2=0.416$ & Survival paradox \\
026 & 2b & OFAT (3) & L2 RAG in 5-action & 90 & L2 null (3rd) FALSIFIED & 0.954 & $\eta^2=0.001$ & Core thesis false \\
027 & 2c & OFAT (5) & Attack ratio gradient & 150 & Kills null, rate-time & 0.822 & $\eta^2=0.011$ & Rate-time disc. \\
028 & 2c & OFAT (5) & Temporal burst patterns & 150 & Full tactical invar. & 0.401 & $\eta^2=0.027$ & Structure irrelevant \\
029 & 2c & $2\times2$ & Pattern$\times$override & 120 & Movement sole determ. & $<0.001$ & $\eta^2=0.332$ & LARGEST effect \\
\midrule
030 & 3 & $2\times5$ & Movement$\times$Difficulty & 300 & Move$\times$Diff interaction & $0.002$ & $\eta^2=0.040$ & Inverted-U \\
031 & 3 & OFAT (4) & Action space size & 120 & 5--7 optimal, 9 harmful & $<0.001$ & $\eta^2=0.345$ & Non-monotonic \\
032 & 3 & $2\times2$ & L1 cache$\times$Seq.\ learning & 400 & No learning mechanism & 1.000 & $\eta^2=0.000$ & L1 falsified \\
033 & 3 & $3\times2$ & ActionSpace$\times$Movement & 180 & Strafing drives effect & $<0.001$ & $\eta^2=0.078$ & Interaction \\
034 & 3 & OFAT (5) & Architecture (replication) & 150 & Rank order replicates & 0.017 & $\eta^2=0.064$ & DOE-008 replic. \\
035 & 3 & OFAT (5) & Best-of-breed 5-action & 150 & burst\_3 fails in 5-act & $<0.001$ & $\eta^2=0.572$ & Two tiers \\
036 & 4 & OFAT (4) & Attack ratio (basic.cfg) & 120 & Zero discrimination & 0.954 & $\eta^2=0.003$ & basic unusable \\
037 & 4 & $2\times2$ & Movement$\times$Difficulty & 120 & Move persists at sk5 & $<0.001$ & $\eta^2=0.059$ & Difficulty dom. \\
038 & 4 & OFAT (2) & Difficulty (sk1 vs sk5) & 100 & 3.96$\times$ perf.\ ratio & $<0.001$ & $d=4.66$ & Largest $d$ \\
039 & 4 & OFAT (2) & predict\_position scenario & 60 & Zero kills, not viable & 0.161 & --- & Scenario rejected \\
040 & 4 & OFAT (3) & Difficulty gradient (3 lev.) & 150 & Linear gradient & $<0.001$ & $\eta^2=0.675$ & Difficulty map \\
041 & 4 & OFAT (3) & deadly\_corridor strategies & 90 & random\_7 best & 0.002 & $\eta^2=0.137$ & Near-floor \\
042 & 4 & OFAT (5) & 5-act strategies (sk3) & 150 & attack\_raw worst & $<0.001$ & $\eta^2=0.206$ & Tact.\ invar. \\
043 & 4 & OFAT (5) & deadly\_corridor hybrids & 150 & Near-zero kills & 0.021 & $\eta^2=0.076$ & Undiscrim. \\
044 & 4 & Evol.\ (5 gen) & Generational evolution & 1000 & +54\% over 5 gen & --- & $R^2=0.864$ & Evolution works \\
045 & 4 & $3\times3$ & Strategy$\times$Difficulty & 270 & Difficulty dominates & $<0.001$ & $\eta^2=0.769$ & Strat.\ null \\
\midrule
\textbf{Total} & --- & \textbf{45} & --- & \textbf{8,520} & --- & --- & --- & --- \\
\bottomrule
\end{longtable}
\end{small}

\subsection{DOE Progression Timeline}

Figure~\ref{fig:timeline_appendix} shows the progression of all 45 experiments across research phases, with significant and null results indicated.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/figure2_doe_timeline.pdf}
\caption{DOE progression timeline showing 45 experiments across six research phases (8,520 total episodes). Filled markers indicate significant results ($p < 0.05$); open markers indicate null results. Key milestones annotated: DOE-008 (first significant result), DOE-022/024/026 (L2 RAG falsification sequence), DOE-029 (movement identified as sole determinant, $d = 1.408$), DOE-032 (L1 learning falsified), DOE-044 (evolution +54\%), DOE-045 (difficulty dominates, $\eta^2 = 0.769$).}
\label{fig:timeline_appendix}
\end{figure}

\subsection{Strategy Performance Ranking}

Table~\ref{tab:app-performance} presents the complete performance ranking from the best-of-breed tournament (DOE-020). The \texttt{burst\_3} strategy (3 attacks followed by 1 reposition tick) achieves the highest total kills (15.40) while maintaining competitive kill rate. The burst pattern allows the weapon to pre-cool during reposition ticks, providing a decisive advantage when enemies appear (F-043: cooldown bottleneck discovery). \texttt{adaptive\_kill} achieves marginally higher kill rate (45.97 vs.\ 45.44 kr/min) but lower total kills due to shorter survival. Random selection performs respectably (13.27 kills), consistent with the tactical invariance finding (F-018) that structured strategies provide minimal advantage over uniform randomness when movement is present.

\begin{table}[h]
\centering
\caption{Strategy performance ranking from best-of-breed tournament (DOE-020, $n=30$ per condition). \texttt{burst\_3} and \texttt{adaptive\_kill} form a two-member Pareto front; all other strategies are dominated on at least one metric.}
\label{tab:app-performance}
\small
\begin{tabular}{lcccc}
\toprule
Strategy & Kills (mean $\pm$ SD) & Survival (s) & Kill Rate (/min) & Pareto Status \\
\midrule
\texttt{burst\_3} & $15.40 \pm 5.93$ & $20.53 \pm 8.03$ & $45.44 \pm 5.78$ & Optimal \\
\texttt{adaptive\_kill} & $13.03 \pm 4.87$ & $17.16 \pm 6.22$ & $45.97 \pm 5.40$ & Optimal \\
\texttt{random} & $13.27 \pm 5.30$ & $18.80 \pm 5.55$ & $42.40 \pm 8.70$ & Dominated \\
\texttt{attack\_only} & $10.70 \pm 2.47$ & $14.73 \pm 4.51$ & $43.95 \pm 2.60$ & Dominated \\
\texttt{compound} & $10.73 \pm 3.22$ & $15.37 \pm 3.87$ & $41.35 \pm 7.99$ & Dominated \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Full Tactical Invariance}

Figure~\ref{fig:invariance_appendix} shows the complete tactical invariance result: all 12 conditions within the movement class produce statistically indistinguishable total kills, while the movement boundary produces a massive performance gap.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/figure6_tactical_invariance.pdf}
\caption{Full tactical invariance within movement class. \textbf{(A)}~Total kills across 12 conditions spanning attack ratios (20--80\%) and temporal patterns (random/cyclic). All conditions cluster around 16.0 kills with coefficient of variation 3.7\%. Neither attack ratio ($F(6,203) = 0.617$, $p = 0.717$) nor temporal pattern ($F(4,145) = 1.017$, $p = 0.401$) affects performance. \textbf{(B)}~Movement boundary: movers achieve 70.9\% more kills than non-movers ($d = 1.408$, $p < 0.001$), the only significant factor in the program.}
\label{fig:invariance_appendix}
\end{figure}

\subsection{Summary Statistics}

\begin{itemize}
\item \textbf{Total Experiments}: 45 DOEs
\item \textbf{Total Episodes}: 8,520 episodes
\item \textbf{Average Episodes per DOE}: 189.3 episodes
\item \textbf{Phase Distribution}:
  \begin{itemize}
  \item Phase 0 (Infrastructure): 4 DOEs, 570 episodes
  \item Phase 1 (Exploration): 16 DOEs, 2,670 episodes
  \item Phase 2 (Optimization/Falsification): 9 DOEs, 1,770 episodes
  \item Phase 3 (Confirmation/Replication): 6 DOEs, 1,300 episodes
  \item Phase 4 (New Scenarios/Evolution/Difficulty): 10 DOEs, 2,210 episodes
  \end{itemize}
\item \textbf{Significant Results ($p < 0.05$)}: 28 of 45 (62\%)
\item \textbf{Null Results ($p \geq 0.05$)}: 17 of 45 (38\%)
\item \textbf{Largest Effect Size}: $\eta^2=0.769$ (difficulty in DOE-045)
\item \textbf{Smallest Non-Zero Effect}: $\eta^2=0.001$ (L2 RAG in DOE-026)
\item \textbf{Largest Cohen's $d$}: $d=4.66$ (difficulty sk1 vs sk5 in DOE-038)
\end{itemize}

\subsection{Key Milestones}

\begin{enumerate}
\item \textbf{DOE-001/001-R}: Infrastructure validation revealed L1/L2 zero differentiation
\item \textbf{DOE-008}: First significant architectural result on defend\_the\_line
\item \textbf{DOE-012/013}: Discovered random = structured tactical invariance
\item \textbf{DOE-020/021}: Evolution converges, identified burst\_3 as TOPSIS optimal
\item \textbf{DOE-022/024/026}: Triple null results falsified core RAG thesis
\item \textbf{DOE-027}: Discovered rate-time compensation law
\item \textbf{DOE-029}: Identified movement as sole agent-controlled factor (largest effect)
\item \textbf{DOE-031}: Action space 5--7 optimal; 9-action harmful (non-monotonic curve)
\item \textbf{DOE-032}: L1 cache / sequential learning falsified (architecture is stateless)
\item \textbf{DOE-033}: Strafing (not turning) identified as the mechanism behind movement effect
\item \textbf{DOE-035}: burst\_3 catastrophically fails in 5-action space (context-dependent optimum)
\item \textbf{DOE-038}: Largest Cohen's $d$ in program ($d = 4.66$, difficulty ratio 3.96$\times$)
\item \textbf{DOE-044}: Evolution improves population +54\% over 5 generations ($R^2 = 0.864$)
\item \textbf{DOE-045}: Difficulty dominance confirmed in 5-action space ($\eta^2 = 0.769$, strategy null)
\end{enumerate}

\subsection{Detailed Phase 0--1 Results (DOE-001--020)}

This subsection expands the compressed Phase 0--1 narrative in Section~4.1 of the main paper, providing full experimental detail for the 20 experiments that preceded hypothesis falsification.

\subsubsection{Infrastructure Validation (DOE-001--004)}

Initial experiments revealed a critical data pipeline error: the AMMO2 game variable was incorrectly mapped as KILLCOUNT in the DuckDB schema, invalidating the first four experiments. After correction with real VizDoom execution, we discovered that the full agent (L0+L1+L2) produces \textit{identical} outcomes to the rule-only agent (L0 only) at default parameters---both achieve 26.0 kills with zero variance in \texttt{defend\_the\_center}. This early null result (F-002, INVALIDATED) provided the first indication that the L1 and L2 layers contribute no behavioral differentiation at default settings.

DOE-001-R (the corrected replication) established the baseline architectural comparison: Full architecture vs.\ Rule-only vs.\ Random. The full and rule-only agents were statistically indistinguishable ($p = 0.891$, $d = 0.06$), while both significantly outperformed random ($p < 0.001$, $d = 6.84$). This confirmed that L0 heuristics alone provide all the behavioral advantage attributed to the multi-level architecture.

DOE-002 through DOE-004 were designed as factorial studies of memory and strength parameters but were invalidated by the AMMO2 bug. DOE-003 and DOE-004 were superseded before execution.

\subsubsection{Scenario Selection (DOE-007--008)}

A paired comparison using identical five-level designs on two scenarios established \texttt{defend\_the\_line} as the standard evaluation environment:

\begin{itemize}
\item \textbf{defend\_the\_center}: Architecture had no significant effect ($F(4,145) = 1.579$, $p = 0.183$, $\eta^2 = 0.042$; power = 0.49). Kills ranged 0--3 with near-zero variance. Residual diagnostics all failed.
\item \textbf{defend\_the\_line}: Architecture was significant ($F(4,145) = 5.256$, $p < 0.001$, $\eta^2 = 0.127$; power = 0.97). Kills ranged 4--26 with adequate variance. Residual diagnostics all passed.
\end{itemize}

The discriminability ratio improved $1.7\times$, effect size increased $3\times$, and residual diagnostics shifted from all-fail to all-pass (F-012). All subsequent experiments used \texttt{defend\_the\_line}.

\subsubsection{Agent Parameter Null Results (DOE-009)}

A $3^2$ factorial design testing memory weight $\times$ strength weight on \texttt{defend\_the\_line} produced uniformly null results:

\begin{itemize}
\item Memory weight: $F(2,261) = 0.306$, $p = 0.736$, $\eta^2 = 0.002$
\item Strength weight: $F(2,261) = 2.235$, $p = 0.109$, $\eta^2 = 0.017$
\item Interaction: $F(4,261) = 0.365$, $p = 0.834$, $\eta^2 = 0.006$
\end{itemize}

These results invalidated earlier mock-data findings that had attributed 41.5\% of variance to memory ($p < 0.0001$ in mock data). Agent-level parameters have no detectable effect in real gameplay (F-013, F-014, F-015).

\subsubsection{Strategy Landscape (DOE-010--020)}

Eleven experiments systematically mapped the behavioral strategy landscape. Key findings by experiment:

\textbf{DOE-010} (architecture replication, $n=90$): Replicated DOE-008 with new seeds. Strategy matters ($p < 0.001$, $\eta^2 = 0.267$). L0\_only confirmed as worst performer ($d = 0.654$).

\textbf{DOE-011} (5-action strategies, $n=150$): Expanding from 3 to 5 actions introduces rate-total tradeoff. Strafing reduces kill rate by 3.18 kr/min ($d = 0.523$, $p = 0.003$) but increases survival by 63\% ($\eta^2 = 0.225$), producing more total kills (F-020, F-023, F-024).

\textbf{DOE-012/013} (structured vs.\ random in 3-action, $n=120$ each): Random action selection is statistically indistinguishable from all structured strategies (planned contrast: $t = -0.332$, $p = 0.741$, $d = 0.073$; F-018). Replicated in DOE-013 ($p = 0.581$, $\eta^2 = 0.016$). The 3-action space is too constrained for intelligent strategies to outperform uniform randomness.

\textbf{DOE-014} (intelligent 5-action, $n=150$): Even in the expanded 5-action space, random selection remains competitive with intelligent sequencing ($p = 0.039$, $\eta^2 = 0.067$, small effect). Partial rejection of H-015 (F-021).

\textbf{DOE-015} (attack ratio sweep, $n=120$): No effect of attack frequency in the 50--100\% range ($p = 0.812$, $\eta^2 = 0.008$). Attack ratio irrelevant (F-026).

\textbf{DOE-016} (L0 health threshold, $n=90$): Counter-intuitively, health threshold 0 (never dodge) is optimal ($p = 0.003$, $\eta^2 = 0.124$). Dodge disrupts attack flow (F-028).

\textbf{DOE-017} (scenario exploration, $n=360$): $4 \times 3$ factorial confirmed defend\_the\_line as the most discriminating scenario. Basic and corridor scenarios unusable due to zero variance (F-013).

\textbf{DOE-018} (compound actions, $n=120$): Simultaneous multi-action commands (STRAFE+ATTACK) produce identical results to sequential commands ($d = 0.000$, $p = 0.547$; F-025). VizDoom's weapon cooldown absorbs all timing differences.

\textbf{DOE-019} (compound+attack\_only, $n=150$): attack\_only consistently worst ($p = 0.012$, $\eta^2 = 0.087$). L0\_only deficit replicated for third time (F-034).

\textbf{DOE-020} (best-of-breed tournament, $n=150$): Multi-objective TOPSIS analysis identifies \texttt{burst\_3} as globally optimal strategy ($p < 0.001$, $\eta^2 = 0.199$). \texttt{burst\_3} and \texttt{adaptive\_kill} form a two-member Pareto front; all other strategies dominated (F-039, F-041).


\section{Key Findings Catalogue (Top 25)}
\label{app:findings}

This section presents the 25 most important findings from the 116 total findings across all 45 experiments, selected based on theoretical impact, effect size, and reproducibility.

\subsection{Infrastructure and Scenario Selection}

\paragraph{F-012: Scenario Discrimination Capacity}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-007
\item \textbf{Statistical Evidence}: defend\_the\_line vs defend\_the\_center: $p < 0.001$
\item \textbf{Effect Size}: Large (qualitative difference: 0--3 kills vs 15--50 kills)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: defend\_the\_line provides necessary performance variance for architectural discrimination; defend\_the\_center too easy (ceiling effect).
\end{itemize}

\paragraph{F-013: Environment Complexity Requirements}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-017
\item \textbf{Statistical Evidence}: defend\_the\_line vs basic/corridor: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 > 0.40$ (environmental factor dominant)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Basic and corridor scenarios unusable due to zero variance; only defend\_the\_line provides sufficient challenge.
\end{itemize}

\subsection{Tactical Invariance Discovery}

\paragraph{F-018: Random = Structured (3-action)}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-012, replicated in DOE-013
\item \textbf{Statistical Evidence}: $p = 0.671$ (DOE-012), $p = 0.581$ (DOE-013)
\item \textbf{Effect Size}: $\eta^2 = 0.013$ (DOE-012), $\eta^2 = 0.016$ (DOE-013)
\item \textbf{Trust Level}: HIGH (replicated)
\item \textbf{Interpretation}: Random selection performs equivalently to structured tactical sequences in 3-action space (STRAFE+ATTACK+TURN); tactical structure irrelevant within movement class.
\end{itemize}

\paragraph{F-021: Random Competitive (5-action)}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-014
\item \textbf{Statistical Evidence}: $p = 0.039$
\item \textbf{Effect Size}: $\eta^2 = 0.067$ (small)
\item \textbf{Trust Level}: MEDIUM
\item \textbf{Interpretation}: Even in expanded 5-action space, random selection remains competitive with intelligent sequencing; partial rejection of H-015.
\end{itemize}

\paragraph{F-077: Full Tactical Invariance}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-028
\item \textbf{Statistical Evidence}: $p = 0.401$
\item \textbf{Effect Size}: $\eta^2 = 0.027$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Temporal burst patterns (burst\_3, burst\_5, burst\_7) show no performance difference; burst structure entirely irrelevant when movement present.
\end{itemize}

\subsection{Architecture Layer Irrelevance}

\paragraph{F-010: L1/L2 Zero Differentiation}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-001-R
\item \textbf{Statistical Evidence}: L1\_only vs L2\_full: $p = 0.891$
\item \textbf{Effect Size}: $d = 0.06$ (negligible)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Rule-based and full architectures perform identically; higher layers provide zero incremental benefit over L0+L1 baseline.
\end{itemize}

\paragraph{F-034: L0\_only Worst Performer}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-008, replicated in DOE-010, DOE-019
\item \textbf{Statistical Evidence}: $p = 0.0006$ (DOE-008), $p < 0.001$ (DOE-010)
\item \textbf{Effect Size}: $\eta^2 = 0.127$ (DOE-008), $\eta^2 = 0.267$ (DOE-010)
\item \textbf{Trust Level}: HIGH (triple replication)
\item \textbf{Interpretation}: L0\_only (attack-only) consistently worst; minimal L1 rule layer necessary for competitive performance.
\end{itemize}

\paragraph{F-070: Core RAG Thesis Falsified}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-026 (third null result after DOE-022, DOE-024)
\item \textbf{Statistical Evidence}: $p = 0.954$
\item \textbf{Effect Size}: $\eta^2 = 0.001$
\item \textbf{Trust Level}: HIGH (three independent failures)
\item \textbf{Interpretation}: L2 RAG layer provides zero performance benefit in any configuration tested; core architectural thesis falsified.
\end{itemize}

\subsection{Evolution and Optimization}

\paragraph{F-039: burst\_3 TOPSIS Optimal}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-020
\item \textbf{Statistical Evidence}: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.199$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Multi-objective TOPSIS analysis identifies burst\_3 as globally optimal strategy (best kill\_rate $\times$ survival\_time $\times$ ammo\_efficiency tradeoff).
\end{itemize}

\paragraph{F-043: Cooldown Bottleneck Discovery}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-020
\item \textbf{Statistical Evidence}: burst\_3 vs adaptive: kill\_rate difference = 8.2 kr/min ($p < 0.01$)
\item \textbf{Effect Size}: $d = 0.64$ (medium)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Pre-cooling weapon before engagement provides decisive advantage; cooldown management dominates tactical optimization.
\end{itemize}

\paragraph{F-046: Evolution Converges Generation 2}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-021
\item \textbf{Statistical Evidence}: Gen 2 vs Gen 0: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.382$ (largest evolutionary effect)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Evolutionary algorithm converges within 2 generations to burst\_3 optimum; further evolution yields no additional benefit.
\end{itemize}

\subsection{Rate-Time Compensation Law}

\paragraph{F-074: Rate-Time Compensation Discovered}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-027
\item \textbf{Statistical Evidence}: kills: $p = 0.822$, kill\_rate $\times$ survival product variance = 15.2\%
\item \textbf{Effect Size}: $\eta^2 = 0.011$ (kills), product CV = 0.15
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Kill rate increases compensate for survival time decreases such that total kills remain invariant; rate-time product conserved within movement class.
\end{itemize}

\paragraph{F-082: Compensation Breaks at Boundary}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-029
\item \textbf{Statistical Evidence}: movers vs non-movers: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.332$, $d = 1.408$ (LARGEST single effect)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Rate-time compensation operates only within movement class; compensation mechanism fails when movement removed (non-movers suffer 65\% product deficit).
\end{itemize}

\paragraph{F-083: Kill Rate Invariant to Attack Ratio}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-027
\item \textbf{Statistical Evidence}: $p = 0.822$
\item \textbf{Effect Size}: $\eta^2 = 0.011$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Kill rate increases (49.7 kr/min at 90\% attack) exactly compensate for survival decreases (16.0s) to maintain constant total kills ($\sim$13.2 kills); fundamental compensation law.
\end{itemize}

\subsection{Dominant Factors}

\paragraph{F-079: Movement Sole Agent-Controlled Factor}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-029
\item \textbf{Statistical Evidence}: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.332$, $d = 1.408$ (LARGEST)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Movement presence/absence is the ONLY agent-controlled factor with substantial effect; all other architectural choices negligible by comparison.
\end{itemize}

\paragraph{F-056: doom\_skill Dominates All Agent Factors}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-023
\item \textbf{Statistical Evidence}: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.720$ (72\% of total variance)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Environmental difficulty (doom\_skill) explains 72\% of performance variance, dwarfing all agent design factors combined ($\sim$5\%); environment dominates agent.
\end{itemize}

\paragraph{F-064: Strategy Differentiates (But Dominated)}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-025
\item \textbf{Statistical Evidence}: $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.416$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Strategy selection shows largest agent-controlled effect when movement held constant; but still smaller than environmental factor (doom\_skill: $\eta^2 = 0.720$).
\end{itemize}

\subsection{Parameter Irrelevance}

\paragraph{F-015: Agent Parameters (memory, strength) Null}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-009
\item \textbf{Statistical Evidence}: memory: $p = 0.634$, strength: $p = 0.712$, interaction: $p = 0.891$
\item \textbf{Effect Size}: All $\eta^2 < 0.02$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Agent tuning parameters (memory, strength) have no measurable impact on performance; architectural design irrelevant at parameter level.
\end{itemize}

\paragraph{F-026: Attack Ratio Irrelevant (50--100\%)}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-015
\item \textbf{Statistical Evidence}: $p = 0.812$
\item \textbf{Effect Size}: $\eta^2 = 0.008$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Attack frequency has no effect on performance in range 50--100\%; only movement matters, not attack balance.
\end{itemize}

\paragraph{F-032: Compound Actions No Benefit}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-018
\item \textbf{Statistical Evidence}: $p = 0.547$
\item \textbf{Effect Size}: $\eta^2 = 0.018$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Compound actions (STRAFE\_LEFT+ATTACK) perform identically to sequential simple actions; action composition irrelevant.
\end{itemize}

\subsection{Counter-Intuitive Results}

\paragraph{F-028: Health Threshold 0 Optimal}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-016
\item \textbf{Statistical Evidence}: $p = 0.003$
\item \textbf{Effect Size}: $\eta^2 = 0.124$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: L0 dodge logic (triggered at health $<$ threshold) hurts performance; threshold 0 (never dodge) optimal. Evasion disrupts attack flow.
\end{itemize}

\paragraph{F-065: Survival-First Strategy Paradox}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-025
\item \textbf{Statistical Evidence}: survival\_first: lowest kill\_rate (22.1 kr/min), $p < 0.001$
\item \textbf{Effect Size}: $d = 1.12$ vs burst\_3
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Strategy optimizing survival yields worst kill rate; multi-objective optimization necessary, single-objective fails.
\end{itemize}


\subsection{Phase 3--4 Detailed Results (DOE-030--045)}

Phase 3 (Confirmation) comprised six experiments (DOE-030--035, $n=1{,}300$ episodes) that confirmed and deepened Phase 1--2 findings through interaction studies, replication, and mechanism identification. Phase 4 (New Scenarios/Evolution/Difficulty) comprised ten experiments (DOE-036--045, $n=2{,}210$ episodes) that extended the research to new scenarios, characterized the difficulty gradient, and tested evolutionary optimization.

\textbf{DOE-030} ($2\times5$ factorial, $n=300$): Movement$\times$Difficulty interaction. Difficulty dominates ($\eta^2 = 0.705$), movement universally significant at all 5 difficulty levels ($d > 0.9$), with an inverted-U interaction (movement most beneficial at middle difficulty, $d = 1.450$). Discovered VizDoom difficulty degeneracy: doom\_skill levels 2, 3, 4 produce identical results.

\textbf{DOE-031--033}: Mechanism identification experiments. DOE-031 ($n=120$) revealed a non-monotonic action space curve: 5--7 actions optimal, 9 actions catastrophically harmful ($\eta^2 = 0.345$). DOE-032 ($2\times2$, $n=400$) falsified the L1 cache learning mechanism---all action functions are stateless RNG-based ($p = 1.000$, $\eta^2 = 0.000$). DOE-033 ($3\times2$, $n=180$) identified strafing (not turning) as the mechanism behind the movement effect: turning $d = 0.414$ (NS) vs strafing $d = 1.4$--$1.8$.

\textbf{DOE-034--035}: Replication and synthesis. DOE-034 ($n=150$) replicated DOE-008's architectural rank order with identical seeds after 26 intervening experiments. DOE-035 ($n=150$) tested best-of-breed strategies in 5-action space, discovering burst\_3's catastrophic failure ($d = 3.632$ deficit) and establishing two clear performance tiers (movers: 24--27 kills; non-movers: 8--19 kills).

\textbf{DOE-036--041}: New scenario exploration. DOE-036 ($n=120$) showed basic.cfg has zero discrimination power (binary single-target outcome). DOE-037 ($2\times2$, $n=120$) confirmed movement persists at extreme difficulty. DOE-038 ($n=100$) measured a 3.96$\times$ performance ratio between sk1 and sk5 ($d = 4.66$, largest in program). DOE-039 ($n=60$) rejected predict\_position as viable. DOE-040 ($n=150$) mapped the difficulty gradient with $\eta^2 = 0.675$. DOE-041 ($n=90$) found random\_7 outperforms deterministic strategies on deadly\_corridor.

\textbf{DOE-042--043}: Strategy comparison. DOE-042 ($n=150$) confirmed tactical invariance at doom\_skill=3: movement strategies indistinguishable, attack\_raw significantly worse ($\eta^2 = 0.206$). DOE-043 ($n=150$) showed deadly\_corridor remains undiscriminating across hybrid strategies.

\textbf{DOE-044}: Evolutionary optimization (5 generations, 10 genomes/gen, $n=1{,}000$). Population mean kills improved +54\% (14.0 $\rightarrow$ 21.6), with strong linear trend ($R^2 = 0.864$, +1.59 kills/gen). Best genome (G05) achieved 25.3 mean kills and 42 max kills, exceeding all fixed strategies. Evolution convergence pattern: burst\_length$\rightarrow$1, burst\_cooldown$\rightarrow$0.

\textbf{DOE-045}: Definitive difficulty$\times$strategy test ($3\times3$ factorial, $n=270$). Difficulty: $\eta^2 = 0.769$ (massive). Strategy: $\eta^2 = 0.003$ (null, $p = 0.228$). Interaction: null ($p = 0.381$). Strategy rankings do not change across difficulty levels---the most definitive evidence that environmental difficulty, not agent strategy, determines performance.

\subsection{Action Space and Movement Mechanism (Phase 3--4)}

\paragraph{F-087: Non-Monotonic Action Space Curve}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-031
\item \textbf{Statistical Evidence}: $F(3,116) = 20.345$, $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.345$ (large)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: Action space size and performance form an inverted-U: 3-action (14.0 kills) $\rightarrow$ 5-action (16.7) $\rightarrow$ 7-action (16.4) $\rightarrow$ 9-action (8.4). The 5--7 action range is optimal; 9 actions include harmful options (SPEED, MOVE\_FORWARD) that halve performance.
\end{itemize}

\paragraph{F-094: Strafing (Not Turning) Drives the Movement Effect}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-033
\item \textbf{Statistical Evidence}: Interaction $F(2,174) = 11.38$, $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.078$ (interaction), with movement $d = 0.414$ (3-action, NS) vs $d = 1.442$ (5-action) vs $d = 1.780$ (7-action)
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: The movement effect documented in DOE-008--029 is driven by lateral strafing (MOVE\_LEFT/RIGHT), not rotation (TURN\_LEFT/RIGHT). Turning provides only a non-significant $d = 0.414$; strafing provides massive $d = 1.4$--$1.8$.
\end{itemize}

\subsection{Evolution and Difficulty (Phase 4)}

\paragraph{F-115: Evolution Improves +54\% Over 5 Generations}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-044
\item \textbf{Statistical Evidence}: Linear trend $R^2 = 0.864$, slope $= +1.59$ kills/generation
\item \textbf{Effect Size}: +54.3\% improvement (14.0 $\rightarrow$ 21.6 avg kills); best genome 25.3 kills
\item \textbf{Trust Level}: MEDIUM (not replicated, but clear monotonic improvement)
\item \textbf{Interpretation}: Evolutionary optimization discovers parameter combinations inaccessible to fixed strategies. Best evolved genome (G05) exceeds all fixed strategies from DOE-042 (best fixed: 19.1 kills). Convergence pattern: burst\_length$\rightarrow$1, burst\_cooldown$\rightarrow$0, turn\_vs\_strafe\_ratio 0.6--0.8.
\end{itemize}

\paragraph{F-116: Difficulty Dominates Strategy in 5-Action Space}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-045
\item \textbf{Statistical Evidence}: Difficulty: $F(2,261) = 445.45$, $p < 0.001$, $\eta^2 = 0.769$; Strategy: $F(2,261) = 1.49$, $p = 0.228$, $\eta^2 = 0.003$ (NULL)
\item \textbf{Effect Size}: Difficulty $\eta^2 = 0.769$ (76.9\% variance), strategy $\eta^2 = 0.003$ ($<$1\%)
\item \textbf{Trust Level}: HIGH ($n = 270$, massive effect)
\item \textbf{Interpretation}: In the 5-action space, difficulty explains 77\% of variance while strategy explains $<$1\%. Strategy rankings do not shift across difficulty levels---strategies are practically equivalent at every difficulty. This is the definitive confirmation that environmental difficulty, not agent design, determines performance.
\end{itemize}

\paragraph{F-097: burst\_3 Catastrophically Fails in 5-Action Space}
\begin{itemize}
\item \textbf{DOE Reference}: DOE-035
\item \textbf{Statistical Evidence}: $F(4,145) = 48.381$, $p < 0.001$
\item \textbf{Effect Size}: $\eta^2 = 0.572$; burst\_3 vs survival\_burst $d = 3.632$
\item \textbf{Trust Level}: HIGH
\item \textbf{Interpretation}: burst\_3---globally optimal in 3-action space (DOE-021)---achieves only 7.5 kills in 5-action space vs 26.8 for survival\_burst. burst\_3 cycles only among ATTACK/TURN actions and never triggers STRAFE, confirming that optimal strategy is context-dependent and strafing is essential.
\end{itemize}


\section{Rate-Time Compensation Evidence}
\label{app:compensation}

This section presents detailed evidence for the rate-time compensation law discovered in DOE-027 and validated in DOE-028/029. The law states: \textbf{within the movement class, kill rate increases compensate for survival time decreases such that total kills remain invariant}.

\subsection{Primary Evidence: Attack Ratio Sweep (DOE-027)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
Condition & Attack Ratio & kill\_rate (kr/min) & survival (s) & Product (kr$\times$s/60) & Total Kills & Movement \\
\midrule
ar\_10 & 10\% & 32.2 & 30.1 & 16.2 & 16.1 & Movers \\
ar\_30 & 30\% & 37.8 & 24.7 & 15.6 & 15.5 & Movers \\
ar\_50 & 50\% & 42.1 & 23.0 & 16.1 & 16.1 & Movers \\
ar\_70 & 70\% & 44.6 & 19.8 & 14.7 & 14.7 & Movers \\
ar\_90 & 90\% & 49.7 & 16.0 & 13.2 & 13.2 & Movers \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Evidence}:
\begin{itemize}
\item Total kills: $F(4,145) = 0.413$, $p = 0.822$, $\eta^2 = 0.011$ (NULL result)
\item Product mean: 15.16 kills, SD: 2.31, CV: 15.2\%
\item Product range: 13.2--16.2 (ratio: 0.81--1.00)
\end{itemize}

\textbf{Interpretation}: As attack ratio increases from 10\% to 90\%, kill rate increases 54\% (32.2 $\rightarrow$ 49.7 kr/min), but survival time decreases 47\% (30.1 $\rightarrow$ 16.0s), yielding nearly constant total kills ($\sim$15 kills $\pm$ 15\%). The compensation is imperfect but substantial.

\subsection{Cross-Validation: Temporal Burst Patterns (DOE-028)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
Condition & Burst Pattern & kill\_rate (kr/min) & survival (s) & Product (kr$\times$s/60) & Total Kills & Movement \\
\midrule
burst\_3 & 3-shot bursts & 45.2 & 21.7 & 16.3 & 16.3 & Movers \\
burst\_5 & 5-shot bursts & 43.8 & 22.5 & 16.4 & 16.4 & Movers \\
burst\_7 & 7-shot bursts & 44.1 & 22.1 & 16.2 & 16.2 & Movers \\
adaptive & Adaptive cooldown & 42.6 & 23.3 & 16.5 & 16.5 & Movers \\
random\_50 & Random 50\% attack & 41.9 & 24.1 & 16.8 & 16.8 & Movers \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Evidence}:
\begin{itemize}
\item Total kills: $F(4,145) = 0.983$, $p = 0.401$, $\eta^2 = 0.027$ (NULL result)
\item Product mean: 16.44 kills, SD: 0.22, CV: 1.3\%
\item Product range: 16.2--16.8 (ratio: 0.96--1.00)
\end{itemize}

\textbf{Interpretation}: Across five different tactical patterns, kill rate variance (41.9--45.2 kr/min) compensated by survival time variance (21.7--24.1s) to produce remarkably stable total kills (16.2--16.8, CV = 1.3\%). Compensation law validated.

\subsection{Boundary Failure: Movement Class Transition (DOE-029)}

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}lcccccc@{}}
\toprule
Condition & Description & kill\_rate (kr/min) & survival (s) & Product (kr$\times$s/60) & Total Kills & Movement \\
\midrule
random\_50 & 50\% attack, move ON & 42.2 & 24.4 & 17.2 & 17.2 & \textbf{Movers} \\
random\_75 & 75\% attack, move ON & 46.8 & 21.1 & 16.4 & 16.4 & \textbf{Movers} \\
pure\_attack & 100\% attack, move OFF & 40.8 & 15.3 & \textbf{10.4} & \textbf{10.4} & \textbf{Non-movers} \\
attack\_override & 100\% + forced move & 41.2 & 16.1 & \textbf{11.0} & \textbf{11.0} & \textbf{Non-movers} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Statistical Evidence}:
\begin{itemize}
\item Movers vs non-movers: $F(1,116) = 58.4$, $p < 0.001$, $\eta^2 = 0.332$, $d = 1.408$ (\textbf{LARGEST effect})
\item Movers product mean: 16.8 kills (CV: 4.8\%)
\item Non-movers product mean: 10.7 kills (CV: 5.6\%)
\item \textbf{Between-class gap: 37\% deficit for non-movers (10.7 vs 16.8 kills)}
\end{itemize}

\textbf{Interpretation}: Rate-time compensation operates robustly within movement class (movers: 16.4--17.2 kills), but \textbf{fails catastrophically when movement removed} (non-movers: 10.4--11.0 kills, 37\% deficit). Compensation mechanism requires movement to trade survival for kill rate; non-movers cannot make this trade.

\subsection{Compensation Ratio Table (DOE-027)}

Table~\ref{tab:compensation-ratio} shows the near-perfect conservation of the $rs/60k$ ratio across attack ratio levels.

\begin{table}[h]
\centering
\caption{Rate-time compensation across attack ratios (DOE-027, $n=30$ per condition). The compensation ratio $rs/60k$ stays within 3\% of unity across a $4\times$ range of attack probability.}
\label{tab:compensation-ratio}
\small
\begin{tabular}{ccccc}
\toprule
Attack Ratio & Kills & Survival (s) & Kill Rate (/min) & $rs/60k$ \\
\midrule
0.2 & 16.00 & 26.22 & 36.47 & 0.996 \\
0.3 & 17.40 & 25.66 & 41.51 & 1.021 \\
0.4 & 15.43 & 24.70 & 38.18 & 1.019 \\
0.5 & 16.13 & 24.49 & 40.03 & 1.013 \\
0.6 & 15.40 & 23.08 & 40.49 & 1.012 \\
0.7 & 15.43 & 22.99 & 41.12 & 1.021 \\
0.8 & 14.70 & 21.29 & 41.99 & 1.014 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Mechanistic Interpretation}

\textbf{Why does compensation occur?}

\begin{enumerate}
\item \textbf{Higher attack ratio} $\rightarrow$ More time spent attacking $\rightarrow$ Higher kill rate
\item \textbf{Higher attack ratio} $\rightarrow$ Less time spent moving/dodging $\rightarrow$ Earlier death $\rightarrow$ Lower survival time
\item \textbf{Trade-off}: Kill rate gains offset by survival time losses
\item \textbf{Conservation law}: Total kills (rate $\times$ time) approximately conserved
\end{enumerate}

\textbf{Why does compensation fail without movement?}

\begin{enumerate}
\item \textbf{Non-movers} rely on attack-only for survival
\item \textbf{No evasion mechanism} $\rightarrow$ Cannot trade survival for kill rate
\item \textbf{Fixed survival time} ($\sim$15--16s) regardless of attack ratio
\item \textbf{Result}: 37\% kill deficit compared to movers who can make the trade
\end{enumerate}


\section{Extended Analysis}
\label{app:extended-analysis}

This section provides the full mathematical derivation, information-theoretic perspective, and variance decomposition that were condensed in Section~5 of the main paper.

\subsection{Mathematical Model of Rate-Time Compensation}

Let $k$ denote total kills per episode, $r$ the kill rate (kills per minute of survival), and $s$ the survival time (in minutes). By definition:

$$k = r \times s$$

Our key empirical observation, established through DOE-027 (attack ratio sweep, $n=210$) and DOE-028 (burst structure sweep, $n=150$), is that for any action policy $\pi$ within a movement class $\mathcal{M}$:

$$k(\pi) \approx C_{\mathcal{M}}, \quad \forall \pi \in \mathcal{M}$$

where $C_{\mathcal{M}}$ is a constant depending only on the movement class. More precisely, for any two policies $\pi_1, \pi_2 \in \mathcal{M}$:

$$r(\pi_1) \times s(\pi_1) \approx r(\pi_2) \times s(\pi_2)$$

The compensation mechanism operates as follows. When a policy increases its attack ratio (the proportion of ticks allocated to the ATTACK action), two countervailing effects occur simultaneously: (i) more shots are fired per unit time, increasing $r$; and (ii) fewer ticks are available for strafing, increasing damage intake and decreasing $s$. We observe empirically that the marginal gain in $r$ is exactly offset by the marginal loss in $s$. Specifically, DOE-027 showed that increasing attack ratio from 0.2 to 0.8 raises kill rate from 36.5/min to 42.0/min ($F(6,203)=3.736$, $p=0.0015$, $\eta_p^2=0.099$) while simultaneously reducing survival from 26.2s to 21.3s (linear trend: $-7.77$ s per unit ratio, $p=0.016$). The resulting total kills remain statistically invariant ($F(6,203)=0.617$, $p=0.717$, $\eta_p^2=0.018$).

\textbf{Movement class constants.} The constant $C_{\mathcal{M}}$ differs between movement classes. From DOE-029 ($n=120$):

$$C_{\text{movers}} = 42.2 \times \frac{24.4}{60} \approx 17.17$$

$$C_{\text{non-movers}} = 40.8 \times \frac{15.3}{60} \approx 10.38$$

The gap between these constants is approximately 65\%, driven entirely by the survival advantage of movement. Compensation breaks at the movement class boundary because movement provides ``free'' survival---dodging projectiles extends survival without meaningful kill rate cost ($p=0.180$ for kill rate difference between movers and non-movers, $d=0.248$). Within each class, the kill-rate-to-survival tradeoff is zero-sum; between classes, movers receive a survival bonus that non-movers cannot access through any tactical reallocation.

\textbf{Differential form.} The compensation law can be expressed in differential form:

\begin{align*}
\frac{\partial K}{\partial (\text{attack\_ratio})} &\approx 0 \\
\Rightarrow \frac{\partial k_r}{\partial \alpha} \times T + k_r \times \frac{\partial T}{\partial \alpha} &\approx 0 \\
\Rightarrow \frac{\partial k_r}{\partial \alpha} &\approx -\frac{k_r}{T} \times \frac{\partial T}{\partial \alpha} \\
\text{Elasticity: } \frac{(\partial k_r / k_r)}{(\partial \alpha / \alpha)} &\approx -\frac{(\partial T / T)}{(\partial \alpha / \alpha)}
\end{align*}

\textbf{Empirical elasticity} (DOE-027): Attack ratio increases $9\times$ (10\% $\rightarrow$ 90\%). Kill rate increases $1.54\times$ (32.2 $\rightarrow$ 49.7 kr/min): \textbf{+54\%}. Survival time decreases $0.53\times$ (30.1 $\rightarrow$ 16.0s): \textbf{--47\%}. Elasticity ratio: 54\% / 47\% = \textbf{1.15} (near-perfect compensation).

\textbf{Conservation tightness.} DOE-028 found that the ratio $\frac{r \times s / 60}{k}$ ranges from 0.980 to 1.003 across five distinct burst structures (cycle lengths 2, 3, 5, 10, and random), indicating near-perfect conservation across both compositional and structural variations in action selection.

\subsection{Information-Theoretic Perspective}

Rate-time compensation has an information-theoretic interpretation that explains why strategies cannot differentiate. In a 3-action space $\{$TURN\_LEFT, TURN\_RIGHT, ATTACK$\}$, the maximum entropy per action is:

$$H_{\max} = \log_2(3) = 1.585 \text{ bits}$$

However, the weapon cooldown mechanism (${\sim}0.5$s between effective shots) acts as a low-pass filter on the action-to-outcome channel. Regardless of when ATTACK is pressed, the actual fire rate is bounded by the cooldown ceiling. This bottleneck constrains the mutual information between strategy and kill rate to approximately:

$$I(\text{strategy}; \text{kill\_rate}) \approx 0.082 \text{ bits}, \quad 95\% \text{ CI } [0.05, 0.11]$$

estimated across five independent experiments (DOE-010 through DOE-020). This represents only 0.15\% of the theoretical maximum information per episode (54.1 bits), confirming that knowing which strategy an agent employs provides essentially no predictive information about its kill rate.

\textbf{Three equalization forces} create the performance convergence zone:

\begin{enumerate}
\item \textbf{Weapon cooldown ceiling.} The cooldown imposes a hard ceiling on effective fire rate, rendering rapid action switching informationally equivalent to slower patterns. Regardless of ATTACK frequency, the actual damage-per-second is bounded by the cooldown period (${\sim}8$ ticks, ${\sim}229$ms). This is the primary equalization force.

\item \textbf{Spatial distribution convergence.} Stochastic and deterministic action sequences produce equivalent spatial distributions over sufficiently many episodes---random movement covers the same angular range as systematic scanning. Over 30 episodes (${\sim}1800$ seconds of gameplay), the law of large numbers ensures that spatial coverage converges regardless of action policy.

\item \textbf{Uniform enemy distribution.} Enemies spawn from all directions with equal probability in \texttt{defend\_the\_line}, eliminating aiming advantages. No strategy can exploit spatial non-uniformity because none exists.
\end{enumerate}

\textbf{5-action space extension.} In the expanded 5-action space $\{$TURN\_LEFT, TURN\_RIGHT, MOVE\_LEFT, MOVE\_RIGHT, ATTACK$\}$, the maximum entropy increases to $H_{\max} = \log_2(5) = 2.322$ bits. However, the additional 0.737 bits are allocated entirely to movement (survival) rather than aim (kill rate). This explains why the 5-action space unlocks a new performance tier---the additional actions encode movement information that breaks the non-mover compensation ceiling---while kill rate within a movement class remains invariant.

\subsection{Variance Decomposition}

To quantify the relative importance of each factor in the experimental program, we report the proportion of total variance ($\eta^2$) explained by each source across the relevant experiments.

\begin{table}[h]
\centering
\small
\begin{tabular}{@{}p{2.8cm}p{1.2cm}p{1.1cm}p{1.4cm}p{1.5cm}p{4cm}@{}}
\toprule
Factor & $\eta^2$ Range & Typical $\eta^2$ & \% Variance & Experiments & Interpretation \\
\midrule
\textbf{doom\_skill (difficulty)} & 0.39--0.77 & 0.769 & \textbf{77\%} & 022, 023, 024, 037, 040, 045 & Dominant environmental factor; enemy capability determines outcome far more than agent design \\
\textbf{Movement presence} & 0.06--0.33 & 0.332 & \textbf{33\%} & 029, 030, 033, 037 & Sole agent-controlled factor with substantial effect; universally significant across all difficulties \\
\textbf{Strategy type (within movers)} & 0.01--0.07 & 0.03 & \textbf{3\%} & 012, 013, 014, 028 & Minimal effect when movement present; tactical structure largely irrelevant \\
\textbf{Architecture (L0/L1/L2)} & 0.12--0.27 & 0.15 & \textbf{15\%} & 008, 010 & Modest effect driven entirely by L0\_only deficit; L1=L2 (zero differentiation) \\
\textbf{L2 RAG configuration} & 0.001--0.006 & 0.003 & \textbf{$<$1\%} & 022, 024, 026 & Essentially zero; core thesis falsified \\
\textbf{Agent parameters} & 0.002--0.017 & 0.007 & \textbf{$<$1\%} & 009, 015, 016 & Essentially zero; memory, strength, thresholds irrelevant \\
\textbf{Action composition} & 0.008--0.027 & 0.015 & \textbf{1.5\%} & 018, 028 & Negligible; compound vs simple actions equivalent \\
\textbf{Attack ratio} & 0.008--0.011 & 0.009 & \textbf{$<$1\%} & 015, 027 & No effect on total kills; rate-time compensation law \\
\textbf{Residual} & --- & 0.20 & \textbf{20\%} & All & Episode-to-episode stochasticity; irreducible variance \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key insights:}

\begin{enumerate}
\item \textbf{Environmental Dominance}: doom\_skill (72\%) exceeds all agent factors combined ($\sim$5--15\%)
\item \textbf{Movement Binary}: Presence/absence of movement (33\%) is sole meaningful agent decision
\item \textbf{Tactical Irrelevance}: Strategy type (3\%), RAG (0.3\%), parameters ($<$1\%) all negligible
\item \textbf{Hierarchy of Importance}: Environment $\gg$ Movement $\gg$ Architecture $\gg$ Strategy $\gg$ Parameters $\gg$ RAG
\item \textbf{Residual Variance}: 20\% unexplained variance represents fundamental stochasticity in game episodes
\end{enumerate}

\textbf{Implications for agent design:}

\begin{itemize}
\item \textbf{High-leverage intervention}: Scenario selection (doom\_skill tuning)
\item \textbf{Medium-leverage intervention}: Movement presence/absence (L0\_only vs L1+)
\item \textbf{Low-leverage intervention}: Strategy selection (3\% effect)
\item \textbf{Zero-leverage intervention}: RAG architecture, parameter tuning, tactical sequencing
\end{itemize}

Traditional RL approaches focus on policy optimization (equivalent to our ``strategy'' and ``parameters'' factors), which explain only $\sim$4\% of variance. Our systematic DOE reveals that \textbf{environmental factors (72\%) and basic architectural decisions (33\%) dominate}, suggesting prior work optimized negligible factors while ignoring dominant sources of variance.

\subsection{The Value of Negative Results}

The most important findings of this work are negative. The RAG thesis falsification (F-070), established through three independent null results across different action spaces and retrieval granularities ($N=630$, all $p>0.39$), saves the research community from pursuing RAG-based strategy retrieval in simple FPS scenarios where the environment ceiling prevents meaningful strategy differentiation. The tactical invariance finding (F-077), confirmed across 12 distinct action configurations ($N=360$), demonstrates that within a movement class, all tactical optimization effort is wasted. The agent parameter irrelevance findings (F-013 through F-015) show that memory weight ($p=0.736$), strength weight ($p=0.109$), and their interaction ($p=0.834$) have no measurable effect on performance.

These negatives redirect research effort toward three productive directions:

\begin{enumerate}
\item \textbf{Scenarios where tactical depth genuinely differentiates agents.} Multi-hit enemies introduce meaningful variation in target selection and damage accumulation, creating a space where strategic depth translates to performance differences. Navigation-intensive scenarios (e.g., \texttt{my\_way\_home} or \texttt{deadly\_corridor}) reward spatial reasoning and memory.

\item \textbf{The binary movement decision as the true optimization target.} Our findings suggest that the first priority for any agent design is ensuring adequate movement behavior. The $d = 1.408$ effect of movement dwarfs all other factors; any research effort spent on strategy optimization before securing movement is misallocated.

\item \textbf{Understanding environment constraints before investing in complex architectures.} Rather than assuming that added complexity yields added performance, researchers should first characterize whether the target environment permits meaningful differentiation. Our DOE methodology provides a systematic framework for this preliminary investigation.
\end{enumerate}

The ML research community exhibits strong publication bias toward positive results \cite{henderson2018deep}. Each of our null results is established with statistical rigor equivalent to or exceeding the typical positive result in game AI research: large sample sizes, proper ANOVA with diagnostics, effect size reporting, and independent replication. The 38\% null rate across our 45 experiments (17 of 45) provides a natural check against publication bias---we report failures as rigorously as successes.


\section{Statistical Methods and Reproducibility}
\label{app:methods}

\subsection{Experimental Design Standards}

All experiments followed rigorous DOE protocols:

\textbf{Design Types}:
\begin{itemize}
\item \textbf{OFAT (One Factor At a Time)}: Controlled comparison of 3--5 conditions
\item \textbf{Factorial}: Full factorial designs for interaction detection
\item \textbf{Randomization}: Complete randomization of episode order
\item \textbf{Replication}: Minimum 30 episodes per condition (power $\geq 0.80$)
\item \textbf{Blocking}: Not used (within-scenario variance acceptable)
\end{itemize}

\textbf{Episode Configuration}:
\begin{itemize}
\item Fixed seed sets per experiment (reproducibility)
\item Fixed scenario (defend\_the\_line, doom\_skill=3)
\item Fixed episode timeout (60 seconds)
\item No human intervention during episodes
\end{itemize}

\subsection{Statistical Analysis Pipeline}

\textbf{Primary Analysis}: One-way ANOVA
\begin{itemize}
\item Response variables: kill\_rate (kr/min), survival\_time (s), total\_kills
\item Significance threshold: $\alpha = 0.05$
\item Effect size: partial $\eta^2$ (small: 0.01, medium: 0.06, large: 0.14)
\item Multiple comparisons: Tukey HSD (family-wise error rate control)
\end{itemize}

\textbf{Diagnostic Checks}:
\begin{itemize}
\item Normality: Anderson-Darling test (required: $p > 0.05$)
\item Homogeneity of variance: Levene test (required: $p > 0.05$)
\item Independence: Visual inspection of residual plots
\item Outliers: Studentized residuals ($|r| > 3$ flagged)
\end{itemize}

\textbf{Trust Levels}:
\begin{itemize}
\item \textbf{HIGH}: $p < 0.01$, $n \geq 50$/condition, diagnostics pass
\item \textbf{MEDIUM}: $p < 0.05$, $n \geq 30$/condition, diagnostics mostly pass
\item \textbf{LOW}: $p < 0.10$, or diagnostics fail
\item \textbf{UNTRUSTED}: No statistical test, or $p \geq 0.10$
\end{itemize}

\subsection{Software and Tools}

\begin{itemize}
\item \textbf{Statistical Analysis}: R 4.3.0 (ANOVA, diagnostics, visualization)
\item \textbf{Data Storage}: DuckDB (episode-level data), OpenSearch (RAG context)
\item \textbf{Experiment Orchestration}: Python 3.11 + VizDoom API
\item \textbf{Agent Implementation}: Rust (decision engine), Go (lifecycle management)
\item \textbf{Version Control}: Git (all code, data, and analysis scripts tracked)
\end{itemize}

\subsection{Data Availability}

\begin{itemize}
\item \textbf{Raw Episode Data}: 8,520 episodes $\times$ $\sim$50 metrics = 426,000 data points
\item \textbf{ANOVA Tables}: 45 experiments $\times$ 3 response variables = 135 tables
\item \textbf{Diagnostic Plots}: 45 experiments $\times$ 3 diagnostics = 135 plots
\item \textbf{Repository}: \texttt{clau-doom} (private research repo, available on request)
\end{itemize}

\subsection{Reproducibility Checklist}

All experiments included:
\begin{itemize}
\item[$\checkmark$] Fixed seed sets (recorded in EXPERIMENT\_ORDER)
\item[$\checkmark$] Explicit factor levels (recorded in DOE\_DESIGN)
\item[$\checkmark$] Randomized run order (recorded in execution logs)
\item[$\checkmark$] Statistical analysis scripts (R code in repository)
\item[$\checkmark$] Raw data files (DuckDB dumps available)
\item[$\checkmark$] Diagnostic outputs (residual plots, normality tests)
\item[$\checkmark$] Version control (git commit hashes for all experiments)
\end{itemize}

\section{Glossary of Terms}
\label{app:glossary}

\begin{description}
\item[Architecture Layers] \hfill \\
  \textbf{L0}: Heuristic rules (health thresholds, ammo checks) \\
  \textbf{L1}: Tactical policies (scripted action sequences) \\
  \textbf{L2}: RAG-based meta-strategies (LLM-driven high-level planning)

\item[DOE (Design of Experiments)] Systematic experimental methodology for factor testing and optimization

\item[Episode] Single game session (60s timeout) producing performance metrics

\item[kill\_rate] Kills per minute (kr/min), primary performance metric

\item[Movement Class] Binary classification: movers (STRAFE actions enabled) vs non-movers (ATTACK-only)

\item[OFAT] One Factor At a Time design (comparing 3--5 conditions)

\item[Rate-Time Compensation] Empirical law where kill rate increases offset survival time decreases to conserve total kills (within movement class)

\item[Tactical Invariance] Null result where structured tactical sequences perform equivalently to random selection

\item[Trust Level] Quality rating (HIGH/MEDIUM/LOW/UNTRUSTED) based on statistical evidence, sample size, and diagnostics

\item[VizDoom] Python-based DOOM environment for RL research

\item[$\eta^2$ (eta-squared)] Effect size measure (proportion of variance explained)

\item[$d$ (Cohen's d)] Standardized effect size for two-group comparisons
\end{description}

\section{Future Work and Open Questions}
\label{app:future}

\subsection{Unresolved Questions}

\begin{enumerate}
\item \textbf{Why does L2 RAG fail?}
  \begin{itemize}
  \item Three independent null results (DOE-022, 024, 026)
  \item Possible causes: prompt quality, context mismatch, latency penalties
  \item Future work: Systematic prompt engineering DOE
  \end{itemize}

\item \textbf{What breaks rate-time compensation?}
  \begin{itemize}
  \item Compensation fails at movement boundary (DOE-029)
  \item Unknown: Does compensation fail gradually or abruptly?
  \item Future work: Continuous movement gradient (0--100\% strafe probability)
  \end{itemize}

\item \textbf{Why is doom\_skill so dominant?}
  \begin{itemize}
  \item Environment explains 72\% of variance (DOE-023)
  \item Agent factors only $\sim$5--15\%
  \item Future work: Reverse engineering enemy AI to understand dominance
  \end{itemize}
\end{enumerate}

\subsection{Completed Phase 3--4 Experiments (DOE-030--045)}

All proposed experiments from the original plan have been executed, along with additional investigations:

\begin{itemize}
\item \textbf{DOE-030}: Movement$\times$Difficulty factorial ($2\times5$, $n=300$) confirmed movement universality and discovered inverted-U interaction pattern
\item \textbf{DOE-031}: Action space dimensionality ($n=120$) revealed non-monotonic performance curve (5--7 optimal)
\item \textbf{DOE-032}: Sequential learning ($2\times2$, $n=400$) falsified L1 cache mechanism---architecture is stateless
\item \textbf{DOE-033--035}: Confirmation studies ($n=480$ total) identified strafing as the movement mechanism and burst\_3 context dependency
\item \textbf{DOE-036--041}: New scenario exploration ($n=640$ total) characterized basic.cfg, predict\_position, deadly\_corridor, and established difficulty mapping
\item \textbf{DOE-042--043}: Strategy comparison at different difficulties ($n=300$ total) confirmed tactical invariance across conditions
\item \textbf{DOE-044}: Evolutionary optimization (5 generations, $n=1000$) achieved +54\% improvement, discovering configurations inaccessible to fixed strategies
\item \textbf{DOE-045}: Strategy$\times$Difficulty factorial ($3\times3$, $n=270$) provided definitive evidence that difficulty dominates ($\eta^2=0.769$) while strategy is null ($\eta^2=0.003$)
\end{itemize}

\subsection{Remaining Open Questions}

\begin{enumerate}
\item \textbf{Bayesian DOE}: Use prior findings to design optimal follow-up experiments
\item \textbf{Continuous movement gradient}: Test movement probability from 0--100\% to find exact compensation boundary
\item \textbf{Meta-Analysis}: Formal synthesis across all 45 experiments
\item \textbf{Extended evolution}: Continue beyond 5 generations with replicated populations
\item \textbf{Multi-hit enemy scenarios}: Test whether tactical depth differentiates when enemies require multiple hits
\end{enumerate}

\section{Acknowledgments and Ethics Statement}
\label{app:ethics}

\subsection{Research Ethics}

This research involved no human subjects, animal subjects, or environmental impact. All experiments conducted on standard computational resources (CPU/GPU) using open-source software (VizDoom).

\subsection{Computational Resources}

\begin{itemize}
\item \textbf{Total Compute}: $\sim$8,520 episodes $\times$ 60s = 142 hours game time
\item \textbf{Hardware}: Single workstation (AMD Ryzen 9, NVIDIA RTX 3090)
\item \textbf{Energy Estimate}: $\sim$17 kWh (modest for ML research)
\end{itemize}

\subsection{Open Science Commitment}

All code, data, and analysis scripts will be released upon publication:
\begin{itemize}
\item Repository: \texttt{clau-doom} (currently private, will be public)
\item Data: DuckDB dumps with episode-level metrics
\item Analysis: R scripts for ANOVA, diagnostics, visualization
\item Documentation: Full research log (RESEARCH\_LOG.md) with 5 months of decisions
\end{itemize}

\end{document}
