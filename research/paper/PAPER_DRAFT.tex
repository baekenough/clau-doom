\documentclass{article}
\PassOptionsToPackage{numbers}{natbib}
\usepackage[preprint]{neurips_2025}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}

\title{Movement Is All You Need: How 45 Systematic Experiments Falsified RAG-Based FPS Game Agent Optimization}

\author{
  Sang Yi \\
  [Affiliation to be added] \\
  \texttt{[email to be added]} \\
  \And
  Claude Code \\
  Anthropic \\
  AI Co-Investigator \\
}

\begin{document}

\maketitle

\begin{abstract}
We apply Design of Experiments (DOE) methodology --- factorial designs, ANOVA with residual diagnostics, and effect size quantification --- to systematically optimize FPS game agents in VizDoom. Across 45 experiments totaling 8,850 episodes and 116 findings, we test the hypothesis that a multi-level RAG architecture improves agent performance through document quality and scoring accuracy. Three independent experiments falsify this hypothesis: L2 retrieval conditions indistinguishable from L0 baselines ($p=0.929$, DOE-022), decision mode producing no effect on kills ($p=0.393$, DOE-024), and RAG selection indistinguishable from fixed strategies ($p=0.935$, DOE-026). Instead, we discover that lateral movement is the sole performance determinant ($F(1,116)=58.40$, $p<0.001$, $\eta_p^2=0.332$, Cohen's $d=1.408$), while scenario difficulty dominates all variance ($\eta_p^2=0.769$, DOE-045). Evolutionary optimization improves population mean kills by 54\% over five generations, yet difficulty explains $>76$\% of variance while strategy explains $<1$\%. We identify rate-time compensation ($\text{kills} \approx k_r \times t_{\text{surv}}$, within movement class) as a fundamental environment constraint explaining why tactical optimization is futile.
\end{abstract}



\section{Introduction}
\label{sec:intro}

Autonomous game agents have become a prominent AI testbed, with FPS environments presenting challenges in real-time decision-making \cite{kempka2016vizdoom}. The dominant paradigm relies on deep RL \cite{mnih2015human, schulman2017proximal}, but practical optimization involves ad-hoc hyperparameter tuning that is statistically inefficient, produces publication bias, and conflates correlation with causation \cite{bergstra2012random}. Design of Experiments (DOE) \cite{montgomery2017doe} provides a principled alternative that decomposes complex systems into orthogonal factors, yet remains underutilized in ML \cite{bouthillier2021accounting}.

We apply DOE to FPS game agents in VizDoom \cite{kempka2016vizdoom}, seeking to understand the causal structure of performance. Our framework follows five phases: (0)~OFAT screening, (1)~factorial analysis, (2)~hypothesis testing, (3)~constraint discovery, and (4)~generalization and evolution.

We implement a four-level hierarchical agent: \textbf{L0} (hardcoded reflexes), \textbf{L1} (DuckDB episode cache), \textbf{L2} (OpenSearch kNN strategy retrieval), and \textbf{L3} (offline LLM retrospection). No real-time LLM inference occurs during gameplay. The core thesis: $\text{Agent Skill} = f(\text{DocQuality}, \text{ScoringAccuracy})$.

Through 45 experiments (8,850 episodes), we discovered this thesis is false. Three independent tests falsified L2 retrieval ($\eta_p^2 < 0.01$ in all). Only lateral movement produced a significant effect ($d=1.408$, $p<0.001$). Rate-time compensation ($\text{kills} \approx k_r \times t_{\text{surv}}$) explains why tactical optimization is futile. Scenario difficulty explains 77\% of variance ($\eta_p^2=0.769$) and movement 33\%, while all agent architecture parameters collectively explain $<5$\%. Evolutionary optimization achieves +54\% improvement over five generations, yet cannot overcome difficulty dominance.

\textbf{Contributions:}
(1)~First application of formal DOE to FPS game AI;
(2)~45 DOEs with 8,850 episodes and 116 findings with complete audit trails;
(3)~identification of rate-time compensation as a fundamental constraint;
(4)~rigorous falsification of RAG-based retrieval through three independent null results;
(5)~demonstration that difficulty dominance ($\eta_p^2=0.769$) renders strategy optimization negligible.

% MOVED TO APPENDIX: Detailed phase descriptions (Phase 0-3), per-level latency budgets, full "What We Actually Found" prose


\section{Related Work}
\label{sec:related}

\textbf{LLM-based game agents.} Recent work explored LLMs in game agents, including GPT-4 on VizDoom with prohibitive latency \cite{dewynter2024doom}, episodic self-reflection \cite{shinn2023reflexion}, lifelong skill accumulation \cite{wang2023voyager}, and hierarchical LLM-RL separation \cite{zhai2024rlgpt, wang2024deps}. Our work differs by using the LLM exclusively offline and rigorously testing---rather than assuming---whether retrieval helps.

\textbf{VizDoom and deep RL.} VizDoom \cite{kempka2016vizdoom} is a standard FPS benchmark. Deep RL achieved strong performance through game-feature prediction \cite{lample2017playing}, predictive learning \cite{dosovitskiy2017learning}, and population-based training reaching human-level play \cite{jaderberg2019human}. These approaches focus on end-to-end optimization without decomposing which factors matter; our DOE fills this gap.

\textbf{DOE in ML.} DOE remains underutilized despite its potential \cite{bouthillier2021accounting, henderson2018deep}. Bayesian optimization \cite{snoek2012practical} and random search \cite{bergstra2012random} find optima without explaining \emph{why}. Our work demonstrates DOE reveals fundamental constraints that ad-hoc tuning would miss.



\section{Methodology}
\label{sec:method}

\subsection{Agent Architecture}

Our agent employs a four-level hierarchy without real-time LLM inference (Figure~\ref{fig:architecture}). \textbf{L0} (Rust) encodes reflexes: dodge at low health, attack nearest enemy. \textbf{L1} retrieves cached behavioral patterns from DuckDB, notably periodic sequences like \texttt{burst\_3}. \textbf{L2} performs $k$NN vector search over strategy documents in OpenSearch, scoring retrieved tactics against the current game state. \textbf{L3} uses Claude Code between episodes to analyze outcomes and generate strategy documents---never during gameplay. A Python binding interfaces with VizDoom's API at 35~fps.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/figure1_architecture.pdf}
\caption{Four-level hierarchical agent architecture. L0 provides a deterministic behavioral floor. L1 stores periodic action patterns. L2 performs $k$NN vector search over strategy documents generated by L3 retrospection. L3 operates offline. The red marker on L2 indicates the falsification result: RAG retrieval produces no benefit ($p > 0.39$ across three tests).}
\label{fig:architecture}
\end{figure}

\subsection{Experimental Framework}

We adopt DOE methodology from quality engineering. Our program follows five phases (Figure~\ref{fig:timeline}): Phase~0 (OFAT screening, DOE-001--010), Phase~1 (factorial analysis, DOE-011--021), Phase~2 (hypothesis testing, DOE-022/024/026), Phase~3 (constraint discovery, DOE-027--035), and Phase~4 (generalization and evolution, DOE-036--045).

Every experiment uses fixed seed protocols (minimum 30 per condition, power $\geq 0.80$), full ANOVA with residual diagnostics (Anderson-Darling normality, Levene equal variance, run-order independence), effect sizes (partial $\eta^2$, Cohen's $d$) with 95\% CIs, trust classification (HIGH/MEDIUM/LOW/UNTRUSTED), and a complete audit trail from hypothesis through finding adoption (see Appendix~E for full statistical methods).

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure2_doe_timeline.pdf}
\caption{DOE progression across five research phases (8,850 total episodes). Filled markers: significant ($p < 0.05$); open: null. Key milestones: DOE-008 (first significant result), DOE-022/024/026 (RAG falsification), DOE-029 (movement, $d=1.408$), DOE-044 (evolution, +54\%), DOE-045 (difficulty dominance, $\eta^2=0.769$).}
\label{fig:timeline}
\end{figure}

\subsection{VizDoom Environment}

We evaluate on \texttt{defend\_the\_line}, where the agent faces continuously spawning single-hit enemies in a corridor and can turn, strafe, and fire. We test 3-action (turn left/right, attack) and 5-action (adding strafe left/right) spaces. Response variables: kills, survival time, and kill rate. Scenario selection was validated empirically: \texttt{defend\_the\_center} produced near-zero variance ($p=0.183$), while \texttt{defend\_the\_line} provided adequate discrimination ($F(4,145)=5.256$, $p<0.001$, $\eta^2=0.127$).



\section{Results}
\label{sec:results}

We present results across five phases spanning 45 DOEs and 8,850 episodes (Appendix~A: complete DOE table; Appendix~B: top 20 findings).

\subsection{Infrastructure Validation and Strategy Exploration (DOE-001--020)}

DOE-001--004 revealed a data pipeline error (AMMO2 mapped as KILLCOUNT), invalidating early experiments but providing the first indication that L1/L2 contribute no behavioral differentiation. DOE-007--008 established \texttt{defend\_the\_line} as the standard scenario ($3\times$ better effect size vs.\ \texttt{defend\_the\_center}). DOE-009 confirmed agent parameters (memory, strength weights) have no effect ($p=0.736$, $p=0.109$). DOE-010--020 mapped the strategy landscape: L0-only is universally worst ($d=0.65$--$1.48$); random matches structured strategies in 3-action space ($p=0.741$); strafing trades kill rate for survival producing more total kills ($\eta^2=0.225$); \texttt{burst\_3} achieves highest kills (15.40), forming a Pareto front with \texttt{adaptive\_kill}.

% MOVED TO APPENDIX: Table 1 (strategy performance ranking), per-strategy detailed statistics, full DOE-010--020 individual results

\subsection{Core Thesis Falsification (DOE-022, DOE-024, DOE-026)}
\label{sec:falsification}

We tested whether L2 RAG retrieval improves performance through three independent experiments.

\textbf{DOE-022} ($N=120$): L2 retrieval provided no benefit over L0 ($p=0.929$, $d=0.189$). High/low-quality documents produced identical outcomes ($d=0.000$). \textbf{DOE-024} ($N=360$): Decision mode had no effect ($F(3,348)=1.001$, $p=0.393$, $\eta^2=0.009$). \textbf{DOE-026} ($N=150$): The smallest effect in the program ($F(4,145)=0.206$, $p=0.935$, $\eta^2=0.006$); RAG was numerically worst (16.57 vs.\ mean 17.15).

Across three tests ($N=630$), L2 RAG produces no measurable benefit. The hypothesis is \textbf{falsified} (Figure~\ref{fig:l2_forest}, Table~\ref{tab:falsification}).

\begin{table}[t]
\centering
\caption{L2 RAG falsification evidence. All tests show negligible effect sizes.}
\label{tab:falsification}
\small
\begin{tabular}{lcccccc}
\toprule
Experiment & Actions & $N$ & Test & $p$ & $\eta_p^2$ & $d$ \\
\midrule
DOE-022 & 3 & 120 & L2 vs.\ L0 & 0.929 & --- & 0.189 \\
DOE-024 & 3 & 360 & $F(3,348)=1.00$ & 0.393 & 0.009 & 0.079 \\
DOE-026 & 5 & 150 & $F(4,145)=0.21$ & 0.935 & 0.006 & $-0.090$ \\
\midrule
\textbf{Combined} & --- & \textbf{630} & --- & $>0.39$ & $<0.01$ & $|d|<0.19$ \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/figure5_l2_rag_forest_plot.pdf}
\caption{Forest plot of L2 RAG effect sizes (Cohen's $d$) with 95\% CIs. All intervals overlap zero.}
\label{fig:l2_forest}
\end{figure}

\subsection{Rate-Time Compensation Discovery (DOE-027, DOE-028)}

Having falsified RAG, we investigated \textit{why} tactical variation produces null results.

\textbf{DOE-027} ($N=210$): Attack probability varied 20--80\%. Kills invariant ($F(6,203)=0.617$, $p=0.717$), but survival decreased ($-7.77$s/10\%, $p=0.016$) while kill rate increased ($F(6,203)=3.736$, $p=0.002$). These cancel precisely: $\text{kills} \approx \text{kill\_rate} \times \text{survival}/60$. Only 6\% variation across $4\times$ attack range.

\textbf{DOE-028} ($N=150$): Five temporal patterns (random, burst 2/3/5/10) at 50\% attack ratio. Kills invariant ($p=0.401$). Compensation ratio $r \times s / 60k$ ranged 0.980--1.003.

Combined evidence ($N=360$) establishes that neither attack proportion nor temporal distribution affects kills---rate-time compensation is a conservation law (Figure~\ref{fig:invariance}).

% MOVED TO APPENDIX: Table 3/4 (rate-time compensation numerical data)

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure6_tactical_invariance.pdf}
\caption{Full tactical invariance. \textbf{(A)}~All 12 conditions within the movement class cluster around 16.0 kills (CV~3.7\%). Neither attack ratio ($p=0.717$) nor temporal pattern ($p=0.401$) affects performance. \textbf{(B)}~Movement boundary: movers achieve 70.9\% more kills ($d=1.408$, $p<0.001$).}
\label{fig:invariance}
\end{figure}

\subsection{Movement as Sole Determinant (DOE-029)}

A $2^2$ factorial crossed action pattern (with/without strafing) $\times$ health override ($N=120$). Movement produced the largest effect in the program: $F(1,116)=58.4$, $p<0.001$, $\eta^2=0.332$, $d=1.408$. Movers: $17.0 \pm 6.6$ kills, 24.4s survival. Non-movers: $10.0 \pm 2.8$ kills, 15.3s survival. Health override: irrelevant ($p=0.378$). Kill rate did \textit{not} differ ($42.2$ vs.\ $40.8$ kr/min, $p=0.180$)---movement provides survival at zero offensive cost because strafing is orthogonal to aiming. Compensation breaks at the boundary: $C_{\text{movers}}=17.17$ vs.\ $C_{\text{non-movers}}=10.38$ (65\% gap). Figure~\ref{fig:movement} shows the factorial results.

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure4_movement_effect.pdf}
\caption{DOE-029 ($2^2$ factorial, $N=120$). (A)~Movers achieve significantly more kills ($d=1.408$). (B)~Kill rate is equivalent ($p=0.180$): movement extends survival without reducing offense. Error bars: $\pm 1$ SEM.}
\label{fig:movement}
\end{figure}



\subsection{Generalization and Evolution (DOE-036--045)}

Phase~4 extended the program to new scenarios, difficulty levels, action spaces, and evolutionary optimization (10 experiments, 2,210 episodes).

\textbf{Difficulty dominance.} DOE-045 ($N=270$, $3\times 3$ factorial) crossed three strategies with three difficulty levels. Difficulty explained 76.9\% of kill variance ($F(2,261)=445.7$, $p<0.001$, $\eta^2=0.769$), while strategy was non-significant ($F(2,261)=1.49$, $p=0.228$, $\eta^2=0.003$) and the interaction was null ($p=0.381$). Strategy rankings do not change across difficulty. DOE-042 ($N=150$) confirmed that the movement advantage persists at intermediate difficulty ($\eta^2=0.206$, driven by the stationary \texttt{attack\_raw} deficit).

\textbf{Evolutionary optimization.} DOE-044 ($N=1{,}000$, 5 generations $\times$ 10 genomes $\times$ 20 episodes) applied a genetic algorithm with TOPSIS multi-objective fitness. Population mean kills improved from 14.0 to 21.6 (+54\%, $R^2=0.864$), with the best genome reaching 25.3 mean kills and a single-episode maximum of 42. Convergence was non-monotonic (Gen~2$\to$3 regression, Gen~5 recovery), characteristic of healthy evolutionary search. Despite this improvement, DOE-045 shows strategy choice remains negligible relative to difficulty.

\textbf{Action space and scenario boundaries.} DOE-043 ($N=150$) found \texttt{deadly\_corridor} undiscriminating across hybrid strategies (all $<$0.5 mean kills). DOE-031 established 5--7 actions as the optimal action space ($\eta^2=0.127$). These results bound the generalizability of our findings to scenarios with adequate performance variance.


\section{Analysis}
\label{sec:analysis}

\textbf{Rate-time compensation.} For any policy $\pi$ within movement class $\mathcal{M}$: $k(\pi) \approx C_{\mathcal{M}}$, where $C_{\text{movers}} \approx 17.17$ and $C_{\text{non-movers}} \approx 10.38$. Increasing attack ratio raises kill rate $r$ but proportionally reduces survival $s$, with $r \times s$ constant (within 1\% across five temporal structures, DOE-028). Compensation breaks at the movement boundary because strafing provides survival without kill rate cost ($p=0.180$). Figure~\ref{fig:rate_time} visualizes the mechanism.

% MOVED TO APPENDIX: Full mathematical derivation, information-theoretic analysis (strategy provides only 0.082 bits about kill rate, 0.15% of theoretical max)

\begin{figure}[t]
\centering
\includegraphics[width=\textwidth]{figures/figure3_rate_time_compensation.pdf}
\caption{Rate-time compensation (DOE-027). (A)~Kill rate increases with attack ratio ($p=0.002$). (B)~Survival decreases ($p=0.016$). (C)~Total kills invariant ($p=0.717$). Error bars: $\pm 1$ SEM ($n=30$).}
\label{fig:rate_time}
\end{figure}

\textbf{Variance decomposition.} \texttt{doom\_skill} explains 77\% of kill variance ($\eta^2=0.769$, DOE-045), movement 33\% ($\eta^2=0.332$, DOE-029), strategy type $<3$\% (DOE-027/028), L2 RAG $<1$\% (DOE-022/024/026), agent parameters $<1$\% (DOE-009). Environment and movement together explain $>80$\% of all variance.

% MOVED TO APPENDIX: Variance decomposition table, information-theoretic analysis


\section{Discussion}
\label{sec:discussion}

Architecture complexity is irrelevant in \texttt{defend\_the\_line}---a scenario-specific result driven by single-hit enemies, open geometry, and predictable spawning that let weapon cooldown bound kill rate while movement provides the only non-compensated axis. Complexity would matter with multi-hit enemies, navigation, or competitive settings. DOE complements RL: it discovers fundamental constraints explaining \textit{why} optimization is futile, while RL optimizes within unconstrained spaces. We recommend DOE as a preliminary tool before expensive RL training.

\textbf{Limitations.} All experiments use single-hit enemies; multi-hit scenarios may differ. We tested discrete 3/5-action spaces; continuous control could reveal finer effects. The Python glue layer may mask timing effects. Most experiments used \texttt{doom\_skill}=3, though DOE-023/040/045 swept all difficulties. Agents use game state variables, not pixels. Our findings are specific to reflexive decision-making domains where physical action dominates cognitive reasoning. In knowledge-intensive settings---such as enterprise multi-agent systems using RAG for document retrieval, customer support, or code generation---the value proposition of retrieval is fundamentally different: natural language reasoning over retrieved context enables nuanced decision-making that our numerical scoring pipeline cannot capture. Generalizing our null result to all RAG applications would be an overreach.

% MOVED TO APPENDIX: Extended discussion of "Value of Negative Results" (covered in intro/conclusion)


\section{Conclusion}
\label{sec:conclusion}

Through 45 experiments (8,850 episodes, 116 findings), we falsified the hypothesis that RAG improves agent performance in VizDoom's \texttt{defend\_the\_line}:
\begin{enumerate}
\item \textbf{Movement is the sole determinant} ($d=1.408$), producing 65\% more kills via survival at negligible kill rate cost.
\item \textbf{Rate-time compensation} constrains tactics: $r \times s \approx C_{\mathcal{M}}$.
\item \textbf{RAG falsified} by three null results ($N=630$, all $p>0.39$).
\item \textbf{Difficulty dominates} ($\eta^2=0.769$), dwarfing all strategy and architecture parameters.
\item \textbf{Evolution helps but cannot overcome difficulty}: +54\% over 5 generations, yet strategy explains $<1$\% of cross-difficulty variance.
\end{enumerate}
DOE reveals constraints that gradient-based optimization cannot discover. The architecture budget is better spent on whether the agent moves.



\begin{thebibliography}{50}
\bibitem{kempka2016vizdoom}
Kempka, M., Wydmuch, M., Runc, G., Toczek, J., and Jaskowski, W. (2016). ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In \textit{IEEE Conference on Computational Intelligence and Games (CIG)}.

\bibitem{dewynter2024doom}
de Wynter, A. (2024). Can GPT Play Doom? GPT-4 and VizDoom. \textit{arXiv preprint}.

\bibitem{shinn2023reflexion}
Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. (2023). Reflexion: Language agents with verbal reinforcement learning. In \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 36.

\bibitem{wang2023voyager}
Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. (2023). Voyager: An open-ended embodied agent with large language models. \textit{Transactions on Machine Learning Research (TMLR)}.

\bibitem{zhai2024rlgpt}
Zhai, Y., Tong, S., Li, X., Cai, M., Qu, Q., Lee, Y. J., and Ma, Y. (2024). Fine-tuning large vision-language models as decision-making agents via reinforcement learning. \textit{arXiv preprint arXiv:2405.10292}.

\bibitem{wang2024deps}
Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., and Liang, Y. (2024). Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. In \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 37.

\bibitem{lample2017playing}
Lample, G. and Chaplot, D. S. (2017). Playing FPS games with deep reinforcement learning. In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 31(1).

\bibitem{dosovitskiy2017learning}
Dosovitskiy, A. and Koltun, V. (2017). Learning to act by predicting the future. In \textit{International Conference on Learning Representations (ICLR)}.

\bibitem{jaderberg2019human}
Jaderberg, M., Czarnecki, W. M., Dunning, I., et al. (2019). Human-level performance in 3D multiplayer games with population-based reinforcement learning. \textit{Science}, 364(6443), 859--865.

\bibitem{montgomery2017doe}
Montgomery, D. C. (2017). \textit{Design and Analysis of Experiments} (9th ed.). John Wiley \& Sons.

\bibitem{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R. P. (2012). Practical Bayesian optimization of machine learning algorithms. In \textit{Advances in Neural Information Processing Systems (NeurIPS)}, 25.

\bibitem{bergstra2012random}
Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. \textit{Journal of Machine Learning Research}, 13, 281--305.

\bibitem{bouthillier2021accounting}
Bouthillier, X., Delaunay, P., Bronzi, M., Trofimov, A., Nichyporuk, B., Szeto, J., Sepah, N., Raff, E., Madan, K., Voleti, V., Kahou, S. E., Michalski, V., Arbel, T., Pal, C., Varoquaux, G., and Vincent, P. (2021). Accounting for variance in machine learning benchmarks. In \textit{Proceedings of Machine Learning and Systems (MLSys)}, 3.

\bibitem{henderson2018deep}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D. (2018). Deep reinforcement learning that matters. In \textit{Proceedings of the AAAI Conference on Artificial Intelligence}, 32(1).

\bibitem{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., et al. (2015). Human-level control through deep reinforcement learning. \textit{Nature}, 518(7540), 529--533.

\bibitem{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. \textit{arXiv preprint arXiv:1707.06347}.

\end{thebibliography}

\begin{ack}
This research was conducted using Claude Code, an AI-assisted development tool by Anthropic. Claude Code served as an active co-investigator in experimental design, statistical analysis, and paper composition.

\paragraph{Reproducibility}
All experiment orders, reports, and statistical analyses are available in the project repository at \url{https://github.com/sangyi/clau-doom}. Fixed seed sets are provided for all 45 DOEs. Appendices~A--O provide the complete DOE summary table, findings catalogue, variance decomposition, rate-time compensation evidence, statistical methods, glossary, future work directions, ethics statement, and supplementary analyses.
\end{ack}

\end{document}
